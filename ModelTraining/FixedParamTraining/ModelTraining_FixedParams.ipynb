{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and graph performance summary statistics #\n",
    "\n",
    "** Author: Andrew Larkin **, Oregon State University College of Public Health and Human Sciences <br>\n",
    "** Date created: ** January 7, 2018\n",
    "\n",
    "### Summary ###\n",
    "Once hyperparameter values have been selected in the hyperparameter tuning script, ModelTraining_FixedParams reads performance dictionaries in pickled format, trains the model with selected hyperparameters, and saves model weights and metadata from the epoch with the lost dev cost after 50000 epochs. This script is also used to create the senstivity models (i.e. those with one of the input features such as hashtags, emoticons, etc. or an output class label removed form the model structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define global variables and constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larkinan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re, string\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input and output filepaths\n",
    "parentFolder = \"C:/Users/larkinan/Desktop/DBTraining/\"\n",
    "dataset = parentFolder + \"preprocessingOutput/\"\n",
    "performFolder = parentFolder + \"modelTrainingPerformance/\"\n",
    "\n",
    "# pickled datasets to load\n",
    "datasetPickleParams = { # where to store datasets for model training on hard disk\n",
    "                        \"trainDictPicklePath\":dataset + \"TrainDict_Jan7_18.p\",\n",
    "    \"devDictPicklePath\":dataset + \"DevDict_Jan7_18.p\",\n",
    "    \"testDictPicklePath\":dataset + \"TestDict_Jan7_18.p\",\n",
    "    \"allDictPicklePath\":dataset + \"allDict.p\",\n",
    "    \"embeddingMatrixPicklePath\":dataset + \"embeddingMatrix.p\",\n",
    "    \"word2IndexPicklePath\":dataset + \"word2Index.p\",\n",
    "    \"NYCDictPicklePath\":dataset + \"NYCDict_Jan7_18.p\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that hyperparams are fixed, set model params as global variables\n",
    "vecDim = 300\n",
    "batchSize = 64\n",
    "numOutcomes = 7\n",
    "numPostLSTMLayers = 2\n",
    "hiddenLayerActivation = 'tanh'\n",
    "hiddenLayerSize = 256\n",
    "learningRate = 0.00009\n",
    "preSoftmaxLayerSize = 14\n",
    "keepRateLSTM = 0.9\n",
    "keepRate = 0.5\n",
    "numEpochs = 100000\n",
    "l2Reg = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled preprocessed data\n",
    "def loadDatasets(pickleParams):\n",
    "    trainDict = pickle.load(open(pickleParams['trainDictPicklePath'],'rb'))\n",
    "    devDict = pickle.load(open(pickleParams['devDictPicklePath'],'rb'))\n",
    "    testDict = pickle.load(open(pickleParams['testDictPicklePath'],'rb'))\n",
    "    NYC_Dict = pickle.load(open(pickleParams['NYCDictPicklePath'],'rb'))\n",
    "    embeddingMatrix = pickle.load(open(pickleParams['embeddingMatrixPicklePath'],'rb'))\n",
    "    word2IndexMap = pickle.load(open(pickleParams['word2IndexPicklePath'],'rb'))\n",
    "    return(trainDict,devDict,testDict,NYC_Dict,embeddingMatrix,word2IndexMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract vectors from dataset dictionary.  \n",
    "def extractDataFromDict(inputDict):\n",
    "    return(inputDict['sent'], inputDict['labels'], inputDict['seqLens'],\n",
    "           inputDict['hash'], inputDict['emot'],inputDict['loc_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  randomly sample record indices \n",
    "def getSampledIndices(dataX,batchSize):\n",
    "    instanceIndices = list(range(len(dataX)))\n",
    "    np.random.shuffle(instanceIndices)\n",
    "    sampledIndices = instanceIndices[:batchSize]\n",
    "    return(sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sampled text and convert to index values using the map\n",
    "def getSampledXVals(sampledIndices,dataX,word2IndexMap):\n",
    "    sampledX = []\n",
    "    for i in sampledIndices:\n",
    "        sent = dataX[i]\n",
    "        tempX = []\n",
    "        for word in sent.split():\n",
    "            \n",
    "            # when applying to datasets other than the train, dev, and test, some words may not be in the dictionary\n",
    "            if(word in word2IndexMap):\n",
    "                tempX.append(word2IndexMap[word])\n",
    "            else:\n",
    "                tempX.append(word2IndexMap['UNK'])\n",
    "        sampledX.append(tempX)\n",
    "    return(sampledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get a random sample for a single epoch or evaluation###\n",
    "**Inputs**: <br>\n",
    "- **batchSize** (int) - number of records to randomly sample <br>\n",
    "- **dataX** (string array) - tweet text for all records <br>\n",
    "- **dataY** (array of 1x7 int arrrays) - each 1x7 int array corresponds to 7 labels for one record <br>\n",
    "- **dataSeqLens** (int array) - number of words in each record\n",
    "- **dataHash** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a hashtag <br>\n",
    "- **dataEmot** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from an emoticon <br>\n",
    "- **dataLoc** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a regional location description <br>\n",
    "- **word2IndexMap** (dict) - dictionary of word:index keys <br>\n",
    "- **numOutcomes** (int) - number of outcomes in the dataset <br>\n",
    "\n",
    "**Outputs**: <br>\n",
    "- **sampledX** (array of int arrays) - word2Index mapped numbers for the words in the sampled tweets <br>\n",
    "- **sampledY** (array of int arrays) - outcome labels for sampled tweets <br>\n",
    "- **samplesdSeqLens** (int array) - length of of sampled tweets <br>\n",
    "- **sampledHash** (array of int arrays) - indicator values of which words in the sampled tweets are hashtags <br>\n",
    "- **sampledEmot** (array of int arrays) - indicator values of which words in the sampled tweets are emoticon descriptions <br>\n",
    "- **sampledLoc** (array of int arrays) - indicator values of which words in the sampled tweets are regional descriptions that use nature-related vocabulary <br>\n",
    "- **sampledIndices** (int array) - sampled record indices in the original dataset <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceBatch(batchSize,dataX,dataY,\n",
    "                       dataSeqlens,dataHash,dataEmot,dataLoc,\n",
    "                       word2IndexMap,numOutcomes):\n",
    "    \n",
    "    sampledIndices = getSampledIndices(dataX,batchSize)\n",
    "    sampledX = getSampledXVals(sampledIndices,dataX,word2IndexMap)\n",
    "    sampledY = np.asarray([dataY[i][0:numOutcomes*2] for i in sampledIndices]).reshape((batchSize, numOutcomes*2))\n",
    "    sampledSeqlens = [dataSeqlens[i] for i in sampledIndices]\n",
    "    sampledHash = np.asarray([dataHash[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(dataHash[0]),1))\n",
    "    sampledEmot = np.asarray([dataEmot[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(dataEmot[0]),1))\n",
    "    sampledLoc = np.asarray([dataLoc[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(dataLoc[0]),1))\n",
    "    \n",
    "    return(sampledX,sampledY,sampledSeqlens,sampledHash,sampledEmot,sampledLoc,sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(numFeatures,embeddingMatrix,trainDict,testDict,word2IndexMap):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        \n",
    "        vocabSize = len(embeddingMatrix)\n",
    "\n",
    "        trainX,trainY,trainSeqlens,trainHash,trainEmot,trainLoc = extractDataFromDict(trainDict)\n",
    "        testX,testY,testSeqlens,testHash,testEmot,testLoc = extractDataFromDict(testDict)\n",
    "\n",
    "        tf.reset_default_graph() \n",
    "\n",
    "        _inputs = tf.placeholder(tf.int32,shape=[None,numFeatures],name=\"featurePlaceholder\")\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabSize,vecDim],name=\"embeddPlaceholder\")\n",
    "\n",
    "        _labels = tf.placeholder(tf.float32,shape=[None,numOutcomes*2],name=\"labelPlaceholder\")\n",
    "        _seqlens = tf.placeholder(tf.int32,shape=[None],name=\"sequenceLengthPlaceholder\")\n",
    "\n",
    "        # setup hashtag, emoticon, and regional indicators\n",
    "        \n",
    "        _hash_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"hashtagPlaceholder\")\n",
    "        _emot_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"emotPlaceholder\")\n",
    "        _loc_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"locPlaceholder\")\n",
    "\n",
    "        embeddings = tf.Variable(tf.constant(0.0,shape=[vocabSize,vecDim]), trainable=False)\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)\n",
    "        embed = tf.nn.embedding_lookup(embeddings,_inputs)\n",
    "        embed2 = tf.concat(values=[embed,_loc_ind,_emot_ind],axis=2)\n",
    "        \n",
    "        # for sensitivity analyses, investigating the impact of removing hash_ind, \n",
    "        # emot_ind, or loc_ind from the model\n",
    "        \n",
    "        #embed2 = tf.concat(values=[embed,_emot_ind,_loc_ind],axis=2)\n",
    "        #embed2 = tf.concat(values=[embed,_emot_ind,_loc_ind],axis=2)\n",
    "        #embed2 = tf.concat(values=[embed,_hash_ind,_loc_ind],axis=2)\n",
    "\n",
    "        \n",
    "        #setup LSTM layers\n",
    "        with tf.name_scope(\"biGRU\"):\n",
    "            \n",
    "            with tf.variable_scope('forward'):\n",
    "                gru_fw_cell = tf.contrib.rnn.LSTMCell(numFeatures,use_peepholes=False)\n",
    "                gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell,output_keep_prob=keepRateLSTM)\n",
    "\n",
    "            with tf.variable_scope('backward'):\n",
    "                gru_bw_cell = tf.contrib.rnn.LSTMCell(numFeatures,use_peepholes=False)\n",
    "                gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell,output_keep_prob=keepRateLSTM)\n",
    "\n",
    "            (output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=gru_fw_cell,\n",
    "                cell_bw=gru_bw_cell,\n",
    "                inputs = embed2,\n",
    "                sequence_length = _seqlens,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            hidden_input = tf.concat(values=[output_state_fw.h,output_state_bw.h],axis=1)\n",
    "            fullLayerDropped = tf.layers.dropout(hidden_input,1-keepRateLSTM)\n",
    "            \n",
    "            # add weights for L2 regularization\n",
    "            LSTM_weights = tf.Variable(tf.truncated_normal([numFeatures*2,numFeatures*2]))\n",
    "            LSTM_bias = tf.Variable(tf.zeros([numFeatures*2]))\n",
    "            LSTM_output = tf.matmul(fullLayerDropped,LSTM_weights) + LSTM_bias\n",
    "            hiddenLayers = []\n",
    "            hiddenWeights = []\n",
    "            hiddenLayers.append(LSTM_output)\n",
    "            hiddenWeights.append(LSTM_weights)\n",
    "\n",
    "            \n",
    "            # setup postLSTM layers\n",
    "            for i in range(numPostLSTMLayers):\n",
    "                tempLayer = tf.layers.dense(\n",
    "                    hiddenLayers[len(hiddenLayers)-1],\n",
    "                    hiddenLayerSize,\n",
    "                    activation=tf.nn.tanh,\n",
    "                    name = \"hidden\" + str(i)\n",
    "                    )\n",
    "                \n",
    "                fullLayerDropped = tf.layers.dropout(tempLayer,1- keepRate)\n",
    "                \n",
    "                # weights for L2 regularization\n",
    "                tempWeights = tf.Variable(tf.truncated_normal([hiddenLayerSize,hiddenLayerSize]))\n",
    "                tempBias = tf.Variable(tf.zeros([hiddenLayerSize]))\n",
    "                outLayer = tf.matmul(fullLayerDropped,tempWeights) + tempBias\n",
    "                hiddenLayers.append(outLayer)\n",
    "                hiddenWeights.append(tempWeights)\n",
    "                \n",
    "            \n",
    "            # create a fully connected layer before the softmax layer\n",
    "            tempLayer = tf.layers.dense(\n",
    "                hiddenLayers[len(hiddenLayers)-1],\n",
    "                preSoftmaxLayerSize,\n",
    "                activation=tf.nn.relu,\n",
    "                name='preSoftmax'\n",
    "            )\n",
    "            \n",
    "            fullLayerDropped = tf.layers.dropout(tempLayer,rate = 1- keepRate)\n",
    "            final_output = fullLayerDropped\n",
    "            \n",
    "            #greenspace is in index 1. Set to 1 for sensitivity analysis of removing greenspace from the set\n",
    "            #of outcomes\n",
    "            startIndex = 0\n",
    "            \n",
    "            \n",
    "            #flatten output and apply softmax function\n",
    "            concatenatedOutput = tf.identity(final_output[:,startIndex*2:(startIndex+1)*2])\n",
    "            concatenatedLabels = tf.identity(_labels[:,startIndex*2:(startIndex+1)*2])\n",
    "            \n",
    "            for i in range(startIndex+1,7):\n",
    "                concatenatedOutput = tf.concat(\n",
    "                    [\n",
    "                        concatenatedOutput,\n",
    "                        tf.identity(final_output[:,(i*2):(i+1)*2])\n",
    "                    ],\n",
    "                    0\n",
    "                )\n",
    "                concatenatedLabels = tf.concat(\n",
    "                    [\n",
    "                        concatenatedLabels,\n",
    "                        tf.identity(_labels[:,(i*2):(i+1)*2])\n",
    "                    ],\n",
    "                    0\n",
    "                )\n",
    "\n",
    "            softmax = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=concatenatedOutput,\n",
    "                labels = concatenatedLabels\n",
    "            )\n",
    "\n",
    "            # define cost function including L2 regularization\n",
    "            \n",
    "            regularization = 0\n",
    "            for i in range(len(hiddenWeights)):\n",
    "                regularization = regularization + tf.nn.l2_loss(hiddenWeights[i])\n",
    "\n",
    "            cost = tf.reduce_mean(softmax)\n",
    "            cost2 = tf.reduce_mean(cost + l2Reg *regularization)\n",
    "            optimizer = tf.train.AdamOptimizer(learningRate).minimize(cost2)\n",
    "\n",
    "            # generate model predictions for all labels\n",
    "            prediction = tf.argmax(concatenatedOutput,1)\n",
    "\n",
    "            # identify correct predictions and calculate accuracy\n",
    "            correct_prediction = tf.reshape(tf.equal(tf.argmax(concatenatedLabels,1),\n",
    "                                                     prediction),[numOutcomes-startIndex,batchSize])\n",
    "            accuracyVector = tf.reduce_mean(tf.cast(correct_prediction,tf.float16)*100,1)   \n",
    "\n",
    "            \n",
    "            # setup tf objects for saving model metadata and best model weights\n",
    "            model_io_params = [_inputs,_labels,_seqlens,_hash_ind,_emot_ind,_loc_ind,prediction,cost]\n",
    "            for save_param in model_io_params:\n",
    "                tf.add_to_collection('model_io',save_param)\n",
    "            model_saver = tf.train.Saver(max_to_keep = 5)\n",
    "            model_saver.export_meta_graph(performFolder + \"model_io.meta\",\n",
    "                                          collection_list = ['model_io'])\n",
    "\n",
    "\n",
    "        # initialize and run training session\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(embedding_init, feed_dict= {embedding_placeholder:embeddingMatrix})\n",
    "\n",
    "            # as training progresses, only save model weights which improve on dev cost.\n",
    "            # start with worst possible cost \n",
    "            bestDevCost = 1\n",
    "\n",
    "                        \n",
    "            for step in range(numEpochs):\n",
    "                x_batch, y_batch, seqlen_batch, hashtag_batch, emot_batch,loc_batch,indexNums = getSentenceBatch(\n",
    "                    batchSize,\n",
    "                    trainX,\n",
    "                    trainY,\n",
    "                    trainSeqlens,\n",
    "                    trainHash,\n",
    "                    trainEmot,\n",
    "                    trainLoc,\n",
    "                    word2IndexMap,\n",
    "                    numOutcomes\n",
    "                )\n",
    "\n",
    "                _, c = sess.run([optimizer, cost],feed_dict={\n",
    "                    _inputs:x_batch,\n",
    "                    _labels:y_batch,\n",
    "                    _seqlens:seqlen_batch,\n",
    "                    _hash_ind:hashtag_batch,\n",
    "                    _emot_ind:emot_batch,\n",
    "                    _loc_ind:loc_batch\n",
    "                }\n",
    "                               )\n",
    "\n",
    "                # evaluate model peformance using dev set 500 epochs.  If performance is new best, then save\n",
    "                # model weights\n",
    "\n",
    "                if step  % 500 == 0 and step > 0:\n",
    "                    x_test,y_test,seqlen_test,hashtag_test,emot_test,loc_test,indexNums = getSentenceBatch(\n",
    "                        5000,\n",
    "                        testX,\n",
    "                        testY,\n",
    "                        testSeqlens,\n",
    "                        testHash,\n",
    "                        testEmot,\n",
    "                        testLoc,\n",
    "                        word2IndexMap,\n",
    "                        numOutcomes\n",
    "                    )    \n",
    "                    batch_pred,c = sess.run(\n",
    "                        [tf.argmax(concatenatedOutput,1),cost],\n",
    "                        feed_dict={_inputs:x_test,\n",
    "                                   _labels:y_test,\n",
    "                                   _seqlens:seqlen_test,\n",
    "                                   _hash_ind:hashtag_test,\n",
    "                                   _emot_ind:emot_test,\n",
    "                                   _loc_ind:loc_test\n",
    "                                  }\n",
    "                    )\n",
    "                    \n",
    "                    if(c < bestDevCost):\n",
    "                        bestDevCost = c\n",
    "                        print(\"new best model: %f\", (c))\n",
    "                        model_saver.save(sess,performFolder + \"model\",global_step=step)\n",
    "                  \n",
    "                    print(\"dev at step %i: %s \" % (step,str(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best model: %f 0.31942517\n",
      "dev at step 500: 0.31942517 \n",
      "new best model: %f 0.25873736\n",
      "dev at step 1000: 0.25873736 \n",
      "new best model: %f 0.23352158\n",
      "dev at step 1500: 0.23352158 \n",
      "new best model: %f 0.21689768\n",
      "dev at step 2000: 0.21689768 \n",
      "new best model: %f 0.20683508\n",
      "dev at step 2500: 0.20683508 \n",
      "new best model: %f 0.20473154\n",
      "dev at step 3000: 0.20473154 \n",
      "new best model: %f 0.1963965\n",
      "dev at step 3500: 0.1963965 \n",
      "dev at step 4000: 0.19952711 \n",
      "new best model: %f 0.19196838\n",
      "dev at step 4500: 0.19196838 \n",
      "new best model: %f 0.19022456\n",
      "dev at step 5000: 0.19022456 \n",
      "dev at step 5500: 0.19192366 \n",
      "new best model: %f 0.18621777\n",
      "dev at step 6000: 0.18621777 \n",
      "dev at step 6500: 0.18914944 \n",
      "dev at step 7000: 0.1871446 \n",
      "new best model: %f 0.18375832\n",
      "dev at step 7500: 0.18375832 \n",
      "new best model: %f 0.1824224\n",
      "dev at step 8000: 0.1824224 \n",
      "new best model: %f 0.17798187\n",
      "dev at step 8500: 0.17798187 \n",
      "new best model: %f 0.17243452\n",
      "dev at step 9000: 0.17243452 \n",
      "new best model: %f 0.16979073\n",
      "dev at step 9500: 0.16979073 \n",
      "new best model: %f 0.16731392\n",
      "dev at step 10000: 0.16731392 \n",
      "new best model: %f 0.16536207\n",
      "dev at step 10500: 0.16536207 \n",
      "dev at step 11000: 0.16757505 \n",
      "new best model: %f 0.16465573\n",
      "dev at step 11500: 0.16465573 \n",
      "new best model: %f 0.16305102\n",
      "dev at step 12000: 0.16305102 \n",
      "dev at step 12500: 0.1681227 \n",
      "new best model: %f 0.16136903\n",
      "dev at step 13000: 0.16136903 \n",
      "dev at step 13500: 0.1616292 \n",
      "dev at step 14000: 0.16221678 \n",
      "new best model: %f 0.16075413\n",
      "dev at step 14500: 0.16075413 \n",
      "new best model: %f 0.15932286\n",
      "dev at step 15000: 0.15932286 \n",
      "dev at step 15500: 0.16008772 \n",
      "dev at step 16000: 0.16031373 \n",
      "new best model: %f 0.13754876\n",
      "dev at step 16500: 0.13754876 \n",
      "new best model: %f 0.13518915\n",
      "dev at step 17000: 0.13518915 \n",
      "new best model: %f 0.13274713\n",
      "dev at step 17500: 0.13274713 \n",
      "dev at step 18000: 0.13368307 \n",
      "new best model: %f 0.1323059\n",
      "dev at step 18500: 0.1323059 \n",
      "new best model: %f 0.13152985\n",
      "dev at step 19000: 0.13152985 \n",
      "dev at step 19500: 0.13494849 \n",
      "new best model: %f 0.1312118\n",
      "dev at step 20000: 0.1312118 \n",
      "new best model: %f 0.1269127\n",
      "dev at step 20500: 0.1269127 \n",
      "new best model: %f 0.124273635\n",
      "dev at step 21000: 0.124273635 \n",
      "dev at step 21500: 0.12785003 \n",
      "new best model: %f 0.12350751\n",
      "dev at step 22000: 0.12350751 \n",
      "dev at step 22500: 0.124642484 \n",
      "dev at step 23000: 0.12526691 \n",
      "dev at step 23500: 0.12580056 \n",
      "dev at step 24000: 0.12439174 \n",
      "new best model: %f 0.121188544\n",
      "dev at step 24500: 0.121188544 \n",
      "new best model: %f 0.12082443\n",
      "dev at step 25000: 0.12082443 \n",
      "dev at step 25500: 0.12137843 \n",
      "dev at step 26000: 0.12220991 \n",
      "dev at step 26500: 0.12308594 \n",
      "dev at step 27000: 0.12191383 \n",
      "new best model: %f 0.119071916\n",
      "dev at step 27500: 0.119071916 \n",
      "new best model: %f 0.11797793\n",
      "dev at step 28000: 0.11797793 \n",
      "dev at step 28500: 0.12112309 \n",
      "dev at step 29000: 0.11823316 \n",
      "dev at step 29500: 0.12466775 \n",
      "new best model: %f 0.11506521\n",
      "dev at step 30000: 0.11506521 \n",
      "dev at step 30500: 0.11865469 \n",
      "dev at step 31000: 0.11945159 \n",
      "dev at step 31500: 0.115525566 \n",
      "dev at step 32000: 0.12358161 \n",
      "dev at step 32500: 0.11825477 \n",
      "dev at step 33000: 0.1164349 \n",
      "dev at step 33500: 0.11671773 \n",
      "dev at step 34000: 0.11647838 \n",
      "dev at step 34500: 0.117254496 \n",
      "dev at step 35000: 0.11613564 \n",
      "dev at step 35500: 0.12067444 \n",
      "dev at step 36000: 0.116889246 \n",
      "new best model: %f 0.11421232\n",
      "dev at step 36500: 0.11421232 \n",
      "dev at step 37000: 0.116345674 \n",
      "dev at step 37500: 0.1163484 \n",
      "dev at step 38000: 0.11524756 \n",
      "dev at step 38500: 0.11807073 \n",
      "dev at step 39000: 0.11566743 \n",
      "dev at step 39500: 0.11810533 \n",
      "dev at step 40000: 0.114829764 \n",
      "dev at step 40500: 0.11731499 \n",
      "dev at step 41000: 0.11499658 \n",
      "dev at step 41500: 0.115183085 \n",
      "dev at step 42000: 0.11468493 \n",
      "dev at step 42500: 0.116604015 \n",
      "dev at step 43000: 0.11888178 \n",
      "dev at step 43500: 0.11909488 \n",
      "dev at step 44000: 0.11596953 \n",
      "new best model: %f 0.1136683\n",
      "dev at step 44500: 0.1136683 \n",
      "dev at step 45000: 0.11467964 \n",
      "dev at step 45500: 0.115756206 \n",
      "dev at step 46000: 0.11447896 \n",
      "dev at step 46500: 0.114244886 \n",
      "dev at step 47000: 0.117978364 \n",
      "dev at step 47500: 0.116938494 \n",
      "dev at step 48000: 0.11600922 \n",
      "dev at step 48500: 0.11908901 \n",
      "dev at step 49000: 0.115178175 \n",
      "dev at step 49500: 0.11777101 \n",
      "dev at step 50000: 0.117005914 \n",
      "dev at step 50500: 0.11854732 \n",
      "dev at step 51000: 0.116708465 \n",
      "dev at step 51500: 0.11757027 \n",
      "dev at step 52000: 0.11850187 \n",
      "dev at step 52500: 0.11809351 \n",
      "dev at step 53000: 0.11749428 \n",
      "dev at step 53500: 0.11932101 \n",
      "dev at step 54000: 0.121921904 \n",
      "dev at step 54500: 0.11821437 \n",
      "dev at step 55000: 0.119319394 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6d4a4761e6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnumFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hash'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcreateModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddingMatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainDict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevDict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2IndexMap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-6d4a4761e6ed>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrainDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNYC_Dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddingMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2IndexMap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadDatasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasetPickleParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnumFeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hash'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcreateModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddingMatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainDict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevDict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2IndexMap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-cb44fa34de0d>\u001b[0m in \u001b[0;36mcreateModel\u001b[1;34m(numFeatures, embeddingMatrix, trainDict, testDict, word2IndexMap)\u001b[0m\n\u001b[0;32m    183\u001b[0m                     \u001b[0m_hash_ind\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mhashtag_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                     \u001b[0m_emot_ind\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0memot_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                     \u001b[0m_loc_ind\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mloc_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m                 }\n\u001b[0;32m    187\u001b[0m                                )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main function\n",
    "def main():\n",
    "    trainDict, devDict, testDict, NYC_Dict,embeddingMatrix, word2IndexMap = loadDatasets(datasetPickleParams)\n",
    "    numFeatures = len(trainDict['hash'][0])\n",
    "    createModel(numFeatures,embeddingMatrix,trainDict,devDict,word2IndexMap)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
