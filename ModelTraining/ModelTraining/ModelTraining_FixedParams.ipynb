{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and graph performance summary statistics #\n",
    "\n",
    "** Author: Andrew Larkin **, Oregon State University College of Public Health and Human Sciences <br>\n",
    "** Date created: ** January 7, 2018\n",
    "\n",
    "### Summary ###\n",
    "Once hyperparameter values have been selected in the hyperparameter tuning script, ModelTraining_FixedParams reads performance dictionaries in pickled format, trains the model with selected hyperparameters, and saves model weights and metadata from the epoch with the lost dev cost after 50000 epochs. This script is also used to create the senstivity models (i.e. those with one of the input features such as hashtags, emoticons, etc. or an output class label removed form the model structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define global variables and constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re, string\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input and output filepaths\n",
    "parentFolder = \"C:/Users/larkinan/Desktop/DBTraining/\"\n",
    "dataset = parentFolder + \"preprocessingOutput/\"\n",
    "performFolder = parentFolder + \"modelTrainingPerformance/\"\n",
    "\n",
    "# pickled datasets to load\n",
    "datasetPickleParams = { # where to store datasets for model training on hard disk\n",
    "                        \"trainDictPicklePath\":dataset + \"trainDict.p\",\n",
    "    \"devDictPicklePath\":dataset + \"devDict.p\",\n",
    "    \"testDictPicklePath\":dataset + \"testDict.p\",\n",
    "    \"allDictPicklePath\":dataset + \"allDict.p\",\n",
    "    \"embeddingMatrixPicklePath\":dataset + \"embeddingMatrix.p\",\n",
    "    \"word2IndexPicklePath\":dataset + \"word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now that hyperparams are fixed, set model params as global variables\n",
    "vocabSize = len(embeddingMatrix)\n",
    "vecDim = 300\n",
    "batchSize = 64\n",
    "numOutcomes = 7\n",
    "numPostLSTMLayers = 2\n",
    "hiddenLayerActivation = 'tanh'\n",
    "hiddenLayerSize = 256\n",
    "learningRate = 0.00009\n",
    "preSoftmaxLayerSize = 14\n",
    "keepRateLSTM = 0.9\n",
    "keepRate = 0.5\n",
    "numEpochs = 100000\n",
    "l2Reg = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled preprocessed data\n",
    "def loadDatasets(pickleParams):\n",
    "    trainDict = pickle.load(open(pickleParams['trainDictPicklePath'],'rb'))\n",
    "    devDict = pickle.load(open(pickleParams['devDictPicklePath'],'rb'))\n",
    "    testDict = pickle.load(open(pickleParams['testDictPicklePath'],'rb'))\n",
    "    NYC_Dict = pickle.load(open(pickleParams['NYC_DictPicklePath'],'rb'))\n",
    "    embeddingMatrix = pickle.load(open(pickleParams['embeddingMatrixPicklePath'],'rb'))\n",
    "    word2IndexMap = pickle.load(open(pickleParams['word2IndexPicklePath'],'rb'))\n",
    "    return(trainDict,devDict,testDict,NYC_Dict,embeddingMatrix,word2IndexMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract vectors from dataset dictionary.  \n",
    "def extractDataFromDict(inputDict):\n",
    "    return(inputDict['sent'], inputDict['labels'], inputDict['seqLens'],\n",
    "           inputDict['hash'], inputDict['emot'],inputDict['loc_ind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get a random sample for a single epoch or evaluation###\n",
    "**Inputs**: <br>\n",
    "- **batchSize** (int) - number of records to randomly sample <br>\n",
    "- **dataX** (string array) - tweet text for all records <br>\n",
    "- **dataY** (array of 1x7 int arrrays) - each 1x7 int array corresponds to 7 labels for one record <br>\n",
    "- **dataSeqLens** (int array) - number of words in each record\n",
    "- **dataHash** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a hashtag <br>\n",
    "- **dataEmot** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from an emoticon <br>\n",
    "- **dataLoc** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a regional location description <br>\n",
    "- **word2IndexMap** (dict) - dictionary of word:index keys <br>\n",
    "- **numOutcomes** (int) - number of outcomes in the dataset <br>\n",
    "\n",
    "**Outputs**: <br>\n",
    "- **sampledX** (array of int arrays) - word2Index mapped numbers for the words in the sampled tweets <br>\n",
    "- **sampledY** (array of int arrays) - outcome labels for sampled tweets <br>\n",
    "- **samplesdSeqLens** (int array) - length of of sampled tweets <br>\n",
    "- **sampledHash** (array of int arrays) - indicator values of which words in the sampled tweets are hashtags <br>\n",
    "- **sampledEmot** (array of int arrays) - indicator values of which words in the sampled tweets are emoticon descriptions <br>\n",
    "- **sampledLoc** (array of int arrays) - indicator values of which words in the sampled tweets are regional descriptions that use nature-related vocabulary <br>\n",
    "- **sampledIndices** (int array) - sampled record indices in the original dataset <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceBatch(batchSize,dataX,dataY,\n",
    "                       dataSeqlens,dataHash,dataEmot,dataLoc,\n",
    "                       word2IndexMap,numOutcomes):\n",
    "    \n",
    "    sampledIndices = getSampledIndices(dataX)\n",
    "    sampledX = getSampledXVals(sampledIndices,dataX,word2IndexMap)\n",
    "    sampledY = np.asarray([dataY[i][0:numOutcomes*2] for i in sampledIndices]).reshape((batchSize, numOutcomes*2))\n",
    "    sampledSeqlens = [dataSeqlens[i] for i in batch]\n",
    "    sampledHash = np.asarray([dataHash[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(hash_data[0]),1))\n",
    "    sampledEmot = np.asarray([emot_data[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(emot_data[0]),1))\n",
    "    sampledLoc = np.asarray([loc_data[i] for i in sampledIndices],dtype=np.float32).reshape((batbatchSize,len(loc_data[0]),1))\n",
    "    \n",
    "    return(sampledX,sampledY,sampledSeqlens,sampledHash,sampledEmot,sampledLoc,sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(numFeatures,embeddingMatrix,trainDict,testDict,word2IndexMap):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "\n",
    "        trainX,trainY,trainSeqlens,trainHash,trainEmot,trainLoc = extractDataFromDict(trainDict)\n",
    "        testX,testY,testSeqlens,testHash,testEmot,testLoc = extractDataFromDict(testDict)\n",
    "\n",
    "        tf.reset_default_graph() \n",
    "\n",
    "        _inputs = tf.placeholder(tf.int32,shape=[None,numFeatures],name=\"featurePlaceholder\")\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabSize,vecDim],name=\"embeddPlaceholder\")\n",
    "\n",
    "        _labels = tf.placeholder(tf.float32,shape=[None,numOutcomes*2],name=\"labelPlaceholder\")\n",
    "        _seqlens = tf.placeholder(tf.int32,shape=[None],name=\"sequenceLengthPlaceholder\")\n",
    "\n",
    "        # setup hashtag, emoticon, and regional indicators\n",
    "        \n",
    "        _hash_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"hashtagPlaceholder\")\n",
    "        _emot_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"emotPlaceholder\")\n",
    "        _loc_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"locPlaceholder\")\n",
    "\n",
    "        embeddings = tf.Variable(tf.constant(0.0,shape=[vocabSize,vecDim]), trainable=False)\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)\n",
    "        embed = tf.nn.embedding_lookup(embeddings,_inputs)\n",
    "        embed2 = tf.concat(values=[embed,_loc_ind,_emot_ind],axis=2)\n",
    "        \n",
    "        # for sensitivity analyses, investigating the impact of removing hash_ind, \n",
    "        # emot_ind, or loc_ind from the model\n",
    "        \n",
    "        #embed2 = tf.concat(values=[embed,_emot_ind,_loc_ind],axis=2)\n",
    "        #embed2 = tf.concat(values=[embed,_emot_ind,_loc_ind],axis=2)\n",
    "        #embed2 = tf.concat(values=[embed,_hash_ind,_loc_ind],axis=2)\n",
    "\n",
    "        \n",
    "        #setup LSTM layers\n",
    "        with tf.name_scope(\"biGRU\"):\n",
    "            \n",
    "            with tf.variable_scope('forward'):\n",
    "                gru_fw_cell = tf.contrib.rnn.LSTMCell(numFeatures,use_peepholes=False)\n",
    "                gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell,output_keep_prob=keepRateLSTM)\n",
    "\n",
    "            with tf.variable_scope('backward'):\n",
    "                gru_bw_cell = tf.contrib.rnn.LSTMCell(numFeatures,use_peepholes=False)\n",
    "                gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell,output_keep_prob=keepRateLSTM)\n",
    "\n",
    "            (output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=gru_fw_cell,\n",
    "                cell_bw=gru_bw_cell,\n",
    "                inputs = embed2,\n",
    "                sequence_length = _seqlens,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            hidden_input = tf.concat(values=[output_state_fw.h,output_state_bw.h],axis=1)\n",
    "            fullLayerDropped = tf.layers.dropout(hidden_input,1-keepRateLSTM)\n",
    "            \n",
    "            # add weights for L2 regularization\n",
    "            LSTM_weights = tf.Variable(tf.truncated_normal([numFeatures*2,numFeatures*2]))\n",
    "            LSTM_bias = tf.Variable(tf.zeros([numFeatures*2]))\n",
    "            LSTM_output = tf.matmul(fullLayerDropped,LSTM_weights) + LSTM_bias\n",
    "            hiddenLayers = []\n",
    "            hiddenWeights = []\n",
    "            hiddenLayers.append(LSTM_output)\n",
    "            hiddenWeights.append(LSTM_weights)\n",
    "\n",
    "            \n",
    "            # setup postLSTM layers\n",
    "            for i in range(numPostLSTMLayers):\n",
    "                tempLayer = tf.layers.dense(\n",
    "                    hiddenLayers[len(hiddenLayers)-1],\n",
    "                    hiddenLayerSize,\n",
    "                    activation=tf.nn.tanh,\n",
    "                    name = \"hidden\" + str(i)\n",
    "                    )\n",
    "                \n",
    "                fullLayerDropped = tf.layers.dropout(tempLayer,1- keepRate)\n",
    "                \n",
    "                # weights for L2 regularization\n",
    "                tempWeights = tf.Variable(tf.truncated_normal([hiddenLayerSize,hiddenLayerSize]))\n",
    "                tempBias = tf.Variable(tf.zeros([hiddenLayerSize]))\n",
    "                outLayer = tf.matmul(fullLayerDropped,tempWeights) + tempBias\n",
    "                hiddenLayers.append(outLayer)\n",
    "                hiddenWeights.append(tempWeights)\n",
    "                \n",
    "            \n",
    "            # create a fully connected layer before the softmax layer\n",
    "            tempLayer = tf.layers.dense(\n",
    "                hiddenLayers[len(hiddenLayers)-1],\n",
    "                preSoftmaxLayerSize,\n",
    "                activation=tf.nn.relu,\n",
    "                name='preSoftmax'\n",
    "            )\n",
    "            \n",
    "            fullLayerDropped = tf.layers.dropout(tempLayer,rate = 1- keepRate)\n",
    "            final_output = fullLayerDropped\n",
    "            \n",
    "            #greenspace is in index 1. Set to 1 for sensitivity analysis of removing greenspace from the set\n",
    "            #of outcomes\n",
    "            startIndex = 0\n",
    "            \n",
    "            \n",
    "            #flatten output and apply softmax function\n",
    "            concatenatedOutput = tf.identity(final_output[:,startIndex*2:(startIndex+1)*2])\n",
    "            concatenatedLabels = tf.identity(_labels[:,startIndex*2:(startIndex+1)*2])\n",
    "            \n",
    "            for i in range(startIndex+1,7):\n",
    "                concatenatedOutput = tf.concat(\n",
    "                    [\n",
    "                        concatenatedOutput,\n",
    "                        tf.identity(final_output[:,(i*2):(i+1)*2])\n",
    "                    ],\n",
    "                    0\n",
    "                )\n",
    "                concatenatedLabels = tf.concat(\n",
    "                    [\n",
    "                        concatenatedLabels,\n",
    "                        tf.identity(_labels[:,(i*2):(i+1)*2])\n",
    "                    ],\n",
    "                    0\n",
    "                )\n",
    "\n",
    "            softmax = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=concatenatedOutput,\n",
    "                labels = concatenatedLabels\n",
    "            )\n",
    "\n",
    "            # define cost function including L2 regularization\n",
    "            \n",
    "            regularization = 0\n",
    "            for i in range(len(hiddenWeights)):\n",
    "                regularization = regularization + tf.nn.l2_loss(hiddenWeights[i])\n",
    "\n",
    "            cost = tf.reduce_mean(softmax)\n",
    "            cost2 = tf.reduce_mean(cost + l2Reg *regularization)\n",
    "            optimizer = tf.train.AdamOptimizer(learningRate).minimize(cost2)\n",
    "\n",
    "            # generate model predictions for all labels\n",
    "            prediction = tf.argmax(concatenatedOutput,1)\n",
    "\n",
    "            # identify correct predictions and calculate accuracy\n",
    "            correct_prediction = tf.reshape(tf.equal(tf.argmax(concatenatedLabels,1),\n",
    "                                                     prediction),[numOutcomes-startIndex,batchSize])\n",
    "            accuracyVector = tf.reduce_mean(tf.cast(correct_prediction,tf.float16)*100,1)   \n",
    "\n",
    "            \n",
    "            # setup tf objects for saving model metadata and best model weights\n",
    "            model_io_params = [_inputs,_labels,_seqlens,_hash_ind,_emot_ind,_loc_ind,prediction,cost]\n",
    "            for save_param in model_io_params:\n",
    "                tf.add_to_collection('model_io',save_param)\n",
    "            model_saver = tf.train.Saver(max_to_keep = 5)\n",
    "            model_saver.export_meta_graph(performFolder + \"model_io.meta\",\n",
    "                                          collection_list = ['model_io'])\n",
    "\n",
    "\n",
    "        # initialize and run training session\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(embedding_init, feed_dict= {embedding_placeholder:embeddingMatrix})\n",
    "\n",
    "            # as training progresses, only save model weights which improve on dev cost.\n",
    "            # start with worst possible cost \n",
    "            bestDevCost = 1\n",
    "\n",
    "                        \n",
    "            for step in range(numEpochs):\n",
    "                x_batch, y_batch, seqlen_batch, hashtag_batch, emot_batch,loc_batch,indexNums = getSentenceBatch(\n",
    "                    batchSize,\n",
    "                    trainX,\n",
    "                    trainY,\n",
    "                    trainSeqlens,\n",
    "                    trainHash,\n",
    "                    trainEmot,\n",
    "                    trainLoc,\n",
    "                    word2IndexMap,\n",
    "                    numOutcomes,\n",
    "                    0\n",
    "                )\n",
    "\n",
    "                _, c = sess.run([optimizer, cost],feed_dict={\n",
    "                    _inputs:x_batch,\n",
    "                    _labels:y_batch,\n",
    "                    _seqlens:seqlen_batch,\n",
    "                    _hash_ind:hashtag_batch,\n",
    "                    _emot_ind:emot_batch,\n",
    "                    _loc_ind:loc_batch\n",
    "                }\n",
    "                               )\n",
    "\n",
    "                # evaluate model peformance using dev set 500 epochs.  If performance is new best, then save\n",
    "                # model weights\n",
    "\n",
    "                if step  % 500 == 0 and step > 0:\n",
    "                    x_test,y_test,seqlen_test,hashtag_test,emot_test,loc_test,indexNums = getSentenceBatch(\n",
    "                        5000,\n",
    "                        testX,\n",
    "                        testY,\n",
    "                        testSeqlens,\n",
    "                        testHash,\n",
    "                        testEmot,\n",
    "                        testLoc,\n",
    "                        word2IndexMap,\n",
    "                        numOutcomes,\n",
    "                        0\n",
    "                    )    \n",
    "                    batch_pred,c = sess.run(\n",
    "                        [tf.argmax(concatenatedOutput,1),cost],\n",
    "                        feed_dict={_inputs:x_test,\n",
    "                                   _labels:y_test,\n",
    "                                   _seqlens:seqlen_test,\n",
    "                                   _hash_ind:hashtag_test,\n",
    "                                   _emot_ind:emot_test,\n",
    "                                   _loc_ind:loc_test\n",
    "                                  }\n",
    "                    )\n",
    "                    \n",
    "                    if(c < bestDevCost):\n",
    "                        bestDevCost = min(c,bestDevCost)\n",
    "                        print(\"new best model: %f\", (c))\n",
    "                        model_saver.save(sess,performFolder + \"model\",global_step=step)\n",
    "                  \n",
    "                    print(\"dev at step %i: %s \" % (step,str(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function\n",
    "def main():\n",
    "    trainDict, devDict, testDict, NYC_Dict,embeddingMatrix, word2IndexMap = loadDatasets(datasetPickleParams)\n",
    "    numFeatures = len(trainDict['hash'][0])\n",
    "    create_model(numFeatures,embeddingMatrix,trainDict,devDict,word2IndexMap)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
