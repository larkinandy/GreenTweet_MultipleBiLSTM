{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and graph performance summary statistics #\n",
    "\n",
    "** Author: Andrew Larkin **, Oregon State University College of Public Health and Human Sciences <br>\n",
    "** Date created: ** January 8, 2019, 2018\n",
    "\n",
    "### Summary ###\n",
    "This script was used to train models with varying hyperparameters, and calculate performance metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - import libraries, define global constants and default model values, and filepaths ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re, string\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input and output filepaths\n",
    "parentFolder = \"C:/Users/larkinan/Desktop/DBTraining/\"\n",
    "dataset = parentFolder + \"preprocessingOutput/\"\n",
    "performFolder = parentFolder + \"modelTrainingPerformance/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters and constants for the deep learning model.  \n",
    "\n",
    "\n",
    "\n",
    "# pickled datasets to load\n",
    "datasetPickleParams = { # where to store datasets for model training on hard disk\n",
    "                        \"trainDictPicklePath\":dataset + \"trainDict.p\",\n",
    "    \"devDictPicklePath\":dataset + \"devDict.p\",\n",
    "    \"testDictPicklePath\":dataset + \"testDict.p\",\n",
    "    \"allDictPicklePath\":dataset + \"allDict.p\",\n",
    "    \"embeddingMatrixPicklePath\":dataset + \"embeddingMatrix.p\",\n",
    "    \"word2IndexPicklePath\":dataset + \"word2Index.p\"\n",
    "}\n",
    " \n",
    "\n",
    "performancePickleParams = {\n",
    "        \"parentFolder\":performFolder,\n",
    "    \"learningRate\":performFolder + \"learningRate\",\n",
    "    \"batchSize\":performFolder + \"batchSize\",\n",
    "    \"postLSTMLayerSize\":performFolder + \"postLSTMLayerSize\",\n",
    "    \"postLSTMLayers\":performFolder + \"postLSTMLayers\",\n",
    "    \"keepProbLSTM\":performFolder + \"keepProbLSTM\",\n",
    "    \"keepProb\":performFolder + \"keepProb\",\n",
    "    \"hiddenLayerActivation\":performFolder + \"activationType\",\n",
    "    \"preSoftmaxSize\":performFolder + \"preSoftmaxSize\",\n",
    "    \"l2Reg\":performFolder + \"l2Reg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled preprocessed data\n",
    "def loadDatasets(pickleParams):\n",
    "    trainDict = pickle.load(open(pickleParams['trainDictPicklePath'],'rb'))\n",
    "    devDict = pickle.load(open(pickleParams['devDictPicklePath'],'rb'))\n",
    "    testDict = pickle.load(open(pickleParams['testDictPicklePath'],'rb'))\n",
    "    NYC_Dict = pickle.load(open(pickleParams['NYCDictPicklePath'],'rb'))\n",
    "    embeddingMatrix = pickle.load(open(pickleParams['embeddingMatrixPicklePath'],'rb'))\n",
    "    word2IndexMap = pickle.load(open(pickleParams['word2IndexPicklePath'],'rb'))\n",
    "    return(trainDict,devDict,testDict,NYC_Dict,embeddingMatrix,word2IndexMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract vectors from dataset dictionary.  \n",
    "def extractDataFromDict(inputDict):\n",
    "    return(inputDict['sent'], inputDict['labels'], inputDict['seqLens'],\n",
    "           inputDict['hash'], inputDict['emot'],inputDict['loc_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  randomly sample record indices \n",
    "def getSampledIndices(dataX,batchSize):\n",
    "    instanceIndices = list(range(len(dataX)))\n",
    "    np.random.shuffle(instanceIndices)\n",
    "    sampledIndices = instanceIndices[:batchSize]\n",
    "    return(sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sampled text and convert to index values using the map\n",
    "def getSampledXVals(sampledIndices,dataX,word2IndexMap):\n",
    "    sampledX = []\n",
    "    for i in sampledIndices:\n",
    "        sent = dataX[i]\n",
    "        tempX = []\n",
    "        for word in sent.split():\n",
    "            \n",
    "            # when applying to datasets other than the train, dev, and test, some words may not be in the dictionary\n",
    "            if(word in word2IndexMap):\n",
    "                tempX.append(word2IndexMap[word])\n",
    "            else:\n",
    "                tempX.append(word2IndexMap['UNK'])\n",
    "        sampledX.append(tempX)\n",
    "    return(sampledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get a random sample for a single epoch or evaluation###\n",
    "**Inputs**: <br>\n",
    "- **batchSize** (int) - number of records to randomly sample <br>\n",
    "- **dataX** (string array) - tweet text for all records <br>\n",
    "- **dataY** (array of 1x7 int arrrays) - each 1x7 int array corresponds to 7 labels for one record <br>\n",
    "- **dataSeqLens** (int array) - number of words in each record\n",
    "- **dataHash** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a hashtag <br>\n",
    "- **dataEmot** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from an emoticon <br>\n",
    "- **dataLoc** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a regional location description <br>\n",
    "- **word2IndexMap** (dict) - dictionary of word:index keys <br>\n",
    "- **numOutcomes** (int) - number of outcomes in the dataset <br>\n",
    "\n",
    "**Outputs**: <br>\n",
    "- **sampledX** (array of int arrays) - word2Index mapped numbers for the words in the sampled tweets <br>\n",
    "- **sampledY** (array of int arrays) - outcome labels for sampled tweets <br>\n",
    "- **samplesdSeqLens** (int array) - length of of sampled tweets <br>\n",
    "- **sampledHash** (array of int arrays) - indicator values of which words in the sampled tweets are hashtags <br>\n",
    "- **sampledEmot** (array of int arrays) - indicator values of which words in the sampled tweets are emoticon descriptions <br>\n",
    "- **sampledLoc** (array of int arrays) - indicator values of which words in the sampled tweets are regional descriptions that use nature-related vocabulary <br>\n",
    "- **sampledIndices** (int array) - sampled record indices in the original dataset <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceBatch(batchSize,dataX,dataY,\n",
    "                       dataSeqlens,dataHash,dataEmot,dataLoc,\n",
    "                       word2IndexMap,numOutcomes):\n",
    "    \n",
    "    sampledIndices = getSampledIndices(dataX,batchSize)\n",
    "    sampledX = getSampledXVals(sampledIndices,dataX,word2IndexMap)\n",
    "    sampledY = np.asarray([dataY[i][0:numOutcomes*2] for i in sampledIndices]).reshape((batchSize, numOutcomes*2))\n",
    "    sampledSeqlens = [dataSeqlens[i] for i in sampledIndices]\n",
    "    sampledHash = np.asarray([dataHash[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(dataHash[0]),1))\n",
    "    sampledEmot = np.asarray([dataEmot[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(dataEmot[0]),1))\n",
    "    sampledLoc = np.asarray([dataLoc[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(dataLoc[0]),1))\n",
    "    \n",
    "    return(sampledX,sampledY,sampledSeqlens,sampledHash,sampledEmot,sampledLoc,sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate TP, FN, FP, FN, F1Score, and MCC\n",
    "def calcPerformanceMetrics(prediction,labels,batchSize):\n",
    "    numOutcomes = int(len(labels[0])/2)\n",
    "    F1ScoreArray = np.zeros((numOutcomes,4))\n",
    "    for i in range(numOutcomes):\n",
    "        subsetLabels = labels[:,i*2]\n",
    "        TP, TN, FP, FN, Recall, Precision, F1Score = [0 for x in range(7)]\n",
    "        predictionSubset = prediction[batchSize*(i):batchSize*(i+1)]\n",
    "        positivePredictionLabels = subsetLabels[np.where( predictionSubset == 0 )] # predictions indicate column - 0 is the column for a positive prediction\n",
    "        \n",
    "        # calc TP and FP\n",
    "        if(len(positivePredictionLabels)>0):\n",
    "            TP = np.sum(np.equal(positivePredictionLabels,1)) # labels with a value of 1 are positive\n",
    "            FP = len(positivePredictionLabels) - TP\n",
    "        \n",
    "        # calc TN and FN\n",
    "        negativePredictionLabels = subsetLabels[np.where( predictionSubset == 1 )] # predictions indicate column - 1 is the column for negative prediction\n",
    "        if(len(negativePredictionLabels) > 0):\n",
    "            TN = np.sum(np.equal(negativePredictionLabels,0))\n",
    "            FN = len(negativePredictionLabels) - TN\n",
    "        if(TP > 0):\n",
    "            Recall = (TP*1.0)/((TP+FN)*1.0)\n",
    "            Precision = (TP*1.0)/((TP+FP)*1.0)\n",
    "            F1Score = 2*(Precision*Recall)/(Precision+Recall)\n",
    "        MCCNumerator = TP*TN-FP*FN\n",
    "        MCCDenomenator = math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "        MCC = (MCCNumerator*1.0)/MCCDenomenator\n",
    "        \n",
    "        F1ScoreArray[i,0] = MCC\n",
    "        F1ScoreArray[i,1] = F1Score\n",
    "        F1ScoreArray[i,2] = Precision\n",
    "        F1ScoreArray[i,3] = Recall\n",
    "        \n",
    "    return(F1ScoreArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate performance metrics for a single epoch\n",
    "def calcPerformanceArrays(metricArrays,batch_pred,y_batch,batchSize,step,c):\n",
    "    metricArrays[0].append(calcMCC(batch_pred,y_batch,batchSize))\n",
    "    metricArrays[1].append(batch_pred)\n",
    "    metricArrays[2].append(y_batch)\n",
    "    metricArrays[3].append(step)\n",
    "    metricArrays[4].append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel(numFeatures,embeddingMatrix,trainDict,testDict,word2IndexMap):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        \n",
    "        vocabSize = len(embeddingMatrix)\n",
    "        vecDim = 300\n",
    "        batchSize = 64\n",
    "        numOutcomes = 7\n",
    "        numPostLSTMLayers = 2\n",
    "        hiddenLayerActivation = 'tanh'\n",
    "        hiddenLayerSize = 256\n",
    "        learningRate = 0.00009\n",
    "        preSoftmaxLayerSize = 14\n",
    "        keepRateLSTM = 0.9\n",
    "        keepRate = 0.5\n",
    "        numEpochs = 100000\n",
    "        l2Reg = 0.001\n",
    "\n",
    "        trainX,trainY,trainSeqlens,trainHash,trainEmot,trainLoc = extractDataFromDict(trainDict)\n",
    "        testX,testY,testSeqlens,testHash,testEmot,testLoc = extractDataFromDict(testDict)\n",
    "\n",
    "        tf.reset_default_graph() \n",
    "\n",
    "        _inputs = tf.placeholder(tf.int32,shape=[None,numFeatures],name=\"featurePlaceholder\")\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [vocabSize,vecDim],name=\"embeddPlaceholder\")\n",
    "\n",
    "        _labels = tf.placeholder(tf.float32,shape=[None,numOutcomes*2],name=\"labelPlaceholder\")\n",
    "        _seqlens = tf.placeholder(tf.int32,shape=[None],name=\"sequenceLengthPlaceholder\")\n",
    "\n",
    "        # setup hashtag, emoticon, and regional indicators\n",
    "        \n",
    "        _hash_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"hashtagPlaceholder\")\n",
    "        _emot_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"emotPlaceholder\")\n",
    "        _loc_ind = tf.placeholder(tf.float32,shape=[None,numFeatures,1],name=\"locPlaceholder\")\n",
    "\n",
    "        embeddings = tf.Variable(tf.constant(0.0,shape=[vocabSize,vecDim]), trainable=False)\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)\n",
    "        embed = tf.nn.embedding_lookup(embeddings,_inputs)\n",
    "        embed2 = tf.concat(values=[embed,_loc_ind,_emot_ind],axis=2)\n",
    "        \n",
    "        #setup LSTM layers\n",
    "        with tf.name_scope(\"biGRU\"):\n",
    "            \n",
    "            with tf.variable_scope('forward'):\n",
    "                gru_fw_cell = tf.contrib.rnn.LSTMCell(numFeatures,use_peepholes=False)\n",
    "                gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell,output_keep_prob=keepRateLSTM)\n",
    "\n",
    "            with tf.variable_scope('backward'):\n",
    "                gru_bw_cell = tf.contrib.rnn.LSTMCell(numFeatures,use_peepholes=False)\n",
    "                gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell,output_keep_prob=keepRateLSTM)\n",
    "\n",
    "            (output_fw, output_bw), (output_state_fw, output_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=gru_fw_cell,\n",
    "                cell_bw=gru_bw_cell,\n",
    "                inputs = embed2,\n",
    "                sequence_length = _seqlens,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            hidden_input = tf.concat(values=[output_state_fw.h,output_state_bw.h],axis=1)\n",
    "            fullLayerDropped = tf.layers.dropout(hidden_input,1-keepRateLSTM)\n",
    "            \n",
    "            # add weights for L2 regularization\n",
    "            LSTM_weights = tf.Variable(tf.truncated_normal([numFeatures*2,numFeatures*2]))\n",
    "            LSTM_bias = tf.Variable(tf.zeros([numFeatures*2]))\n",
    "            LSTM_output = tf.matmul(fullLayerDropped,LSTM_weights) + LSTM_bias\n",
    "            hiddenLayers = []\n",
    "            hiddenWeights = []\n",
    "            hiddenLayers.append(LSTM_output)\n",
    "            hiddenWeights.append(LSTM_weights)\n",
    "\n",
    "            \n",
    "            # setup postLSTM layers\n",
    "            for i in range(numPostLSTMLayers):\n",
    "                tempLayer = tf.layers.dense(\n",
    "                    hiddenLayers[len(hiddenLayers)-1],\n",
    "                    hiddenLayerSize,\n",
    "                    activation=tf.nn.tanh,\n",
    "                    name = \"hidden\" + str(i)\n",
    "                    )\n",
    "                \n",
    "                fullLayerDropped = tf.layers.dropout(tempLayer,1- keepRate)\n",
    "                \n",
    "                # weights for L2 regularization\n",
    "                tempWeights = tf.Variable(tf.truncated_normal([hiddenLayerSize,hiddenLayerSize]))\n",
    "                tempBias = tf.Variable(tf.zeros([hiddenLayerSize]))\n",
    "                outLayer = tf.matmul(fullLayerDropped,tempWeights) + tempBias\n",
    "                hiddenLayers.append(outLayer)\n",
    "                hiddenWeights.append(tempWeights)\n",
    "                \n",
    "            \n",
    "            # create a fully connected layer before the softmax layer\n",
    "            tempLayer = tf.layers.dense(\n",
    "                hiddenLayers[len(hiddenLayers)-1],\n",
    "                preSoftmaxLayerSize,\n",
    "                activation=tf.nn.relu,\n",
    "                name='preSoftmax'\n",
    "            )\n",
    "            \n",
    "            fullLayerDropped = tf.layers.dropout(tempLayer,rate = 1- keepRate)\n",
    "            final_output = fullLayerDropped\n",
    "            \n",
    "            #greenspace is in index 1. Set to 1 for sensitivity analysis of removing greenspace from the set\n",
    "            #of outcomes\n",
    "            startIndex = 0\n",
    "            \n",
    "            \n",
    "            #flatten output and apply softmax function\n",
    "            concatenatedOutput = tf.identity(final_output[:,startIndex*2:(startIndex+1)*2])\n",
    "            concatenatedLabels = tf.identity(_labels[:,startIndex*2:(startIndex+1)*2])\n",
    "            \n",
    "            for i in range(startIndex+1,7):\n",
    "                concatenatedOutput = tf.concat(\n",
    "                    [\n",
    "                        concatenatedOutput,\n",
    "                        tf.identity(final_output[:,(i*2):(i+1)*2])\n",
    "                    ],\n",
    "                    0\n",
    "                )\n",
    "                concatenatedLabels = tf.concat(\n",
    "                    [\n",
    "                        concatenatedLabels,\n",
    "                        tf.identity(_labels[:,(i*2):(i+1)*2])\n",
    "                    ],\n",
    "                    0\n",
    "                )\n",
    "\n",
    "            softmax = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits=concatenatedOutput,\n",
    "                labels = concatenatedLabels\n",
    "            )\n",
    "\n",
    "            # define cost function including L2 regularization\n",
    "            \n",
    "            regularization = 0\n",
    "            for i in range(len(hiddenWeights)):\n",
    "                regularization = regularization + tf.nn.l2_loss(hiddenWeights[i])\n",
    "\n",
    "            cost = tf.reduce_mean(softmax)\n",
    "            cost2 = tf.reduce_mean(cost + l2Reg *regularization)\n",
    "            optimizer = tf.train.AdamOptimizer(learningRate).minimize(cost2)\n",
    "\n",
    "            # generate model predictions for all labels\n",
    "            prediction = tf.argmax(concatenatedOutput,1)\n",
    "\n",
    "            # identify correct predictions and calculate accuracy\n",
    "            correct_prediction = tf.reshape(tf.equal(tf.argmax(concatenatedLabels,1),\n",
    "                                                     prediction),[numOutcomes-startIndex,batchSize])\n",
    "            accuracyVector = tf.reduce_mean(tf.cast(correct_prediction,tf.float16)*100,1)   \n",
    "\n",
    "\n",
    "\n",
    "        # initialize and run training session\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(embedding_init, feed_dict= {embedding_placeholder:embeddingMatrix})\n",
    "\n",
    "            trainArrays = [[],[],[],[],[]]\n",
    "            devArrays = [[],[],[],[],[]]\n",
    "\n",
    "                        \n",
    "            for step in range(numEpochs):\n",
    "                x_batch, y_batch, seqlen_batch, hashtag_batch, emot_batch,loc_batch,indexNums = getSentenceBatch(\n",
    "                    batchSize,\n",
    "                    trainX,\n",
    "                    trainY,\n",
    "                    trainSeqlens,\n",
    "                    trainHash,\n",
    "                    trainEmot,\n",
    "                    trainLoc,\n",
    "                    word2IndexMap,\n",
    "                    numOutcomes\n",
    "                )\n",
    "\n",
    "                _, c = sess.run([optimizer, cost],feed_dict={\n",
    "                    _inputs:x_batch,\n",
    "                    _labels:y_batch,\n",
    "                    _seqlens:seqlen_batch,\n",
    "                    _hash_ind:hashtag_batch,\n",
    "                    _emot_ind:emot_batch,\n",
    "                    _loc_ind:loc_batch\n",
    "                }\n",
    "                               )\n",
    "\n",
    "                # evaluate model peformance and save results\n",
    "\n",
    "                if step  % 500 == 0 and step > 0:\n",
    "                    \n",
    "                    \n",
    "                    batch_pred, c = sess.run(\n",
    "                        [tf.argmax(concatenatedOutput,1),cost],\n",
    "                        feed_dict={\n",
    "                            _inputs:x_batch,\n",
    "                            _labels:y_batch,\n",
    "                            _seqlens:seqlen_batch,\n",
    "                            _hash_ind:hashtag_batch,\n",
    "                            _emot_ind:emot_batch,\n",
    "                            _loc_ind:loc_batch\n",
    "                        }\n",
    "                    )\n",
    "                    calcPerformanceArrays(trainArrays,batch_pred,y_batch,batchSize,step,c)\n",
    "                    \n",
    "                    x_test,y_test,seqlen_test,hashtag_test,emot_test,loc_test,indexNums = getSentenceBatch(\n",
    "                        5000,\n",
    "                        testX,\n",
    "                        testY,\n",
    "                        testSeqlens,\n",
    "                        testHash,\n",
    "                        testEmot,\n",
    "                        testLoc,\n",
    "                        word2IndexMap,\n",
    "                        numOutcomes\n",
    "                    )    \n",
    "                    \n",
    "                    batch_pred,c = sess.run(\n",
    "                        [tf.argmax(concatenatedOutput,1),cost],\n",
    "                        feed_dict={\n",
    "                            _inputs:x_test,\n",
    "                            _labels:y_test,\n",
    "                            _seqlens:seqlen_test,\n",
    "                            _hash_ind:hashtag_test,\n",
    "                            _emot_ind:emot_test,\n",
    "                            _loc_ind:loc_test\n",
    "                        }\n",
    "                    )\n",
    "                    calcPerformanceArrays(devArrays,batch_pred,y_test,batchSize,step,c)\n",
    "                    \n",
    "                    print(\"dev at step %i: %s \" % (step,str(c)))\n",
    "                    \n",
    "            writePeformanceMetrics(trainArrays,devArrays,modelParams,pickleParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function\n",
    "def main():\n",
    "    trainDict, devDict, testDict, NYC_Dict,embeddingMatrix, word2IndexMap = loadDatasets(datasetPickleParams)\n",
    "    numFeatures = len(trainDict['hash'][0])\n",
    "    createModel(numFeatures,embeddingMatrix,trainDict,devDict,word2IndexMap)\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
