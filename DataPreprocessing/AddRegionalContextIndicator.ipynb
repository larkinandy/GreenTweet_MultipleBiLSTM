{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for preprocessing tweets for bi-LSTM training #\n",
    "\n",
    "**Author:** [Andrew Larkin](https://www.linkedin.com/in/andrew-larkin-525ba3b5/) <br>\n",
    "**Affiliation:** [Oregon State University, College of Public Health and Human Sciences](https://health.oregonstate.edu/) <br>\n",
    "**Date Created:** January 7, 2019 <br>\n",
    "\n",
    "**Summary** <br>\n",
    "Add a regional context indicator variable for words that have regional meaning.  The regional context indicator was added after datasets were preprocssed, which is why these functions are in a separate script from the PreprocessTweets notebook.  Regional indicators are the word indices within a tweet where a key reigonal indicator phrase is present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup: import packages, define filepaths and global constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as ps\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input and output filepaths\n",
    "parentFolder = \"C:/Users/larkinan/Desktop/DBTraining/\"\n",
    "dataset = parentFolder + \"preprocessingOutput/\"\n",
    "performFolder = parentFolder + \"modelTrainingPerformance/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickled datasets to load\n",
    "datasetPickleParams = { \n",
    "    \"trainDictPicklePath\":dataset + \"trainDict.p\",\n",
    "    \"devDictPicklePath\":dataset + \"devDict.p\",\n",
    "    \"testDictPicklePath\":dataset + \"testDict.p\",\n",
    "    \"allDictPicklePath\":dataset + \"allDict.p\",\n",
    "    \"embeddingMatrixPicklePath\":dataset + \"embeddingMatrix.p\",\n",
    "    \"word2IndexPicklePath\":dataset + \"word2Index.p\",\n",
    "    \"NYC_DictPicklePath\":dataset + \"NYCDict.p\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words in Portland that have regionally unique context\n",
    "keyWordsPDX = ['Enchanted Forest','Splash Mountain','Bull Mountain','Sun River','Lake Oswego','Menlo Park','Forest Heights','Salt Lake',\n",
    "               'Fire on the Mountain','Lake Tahoe','Garden State','CenturyLink Field','Mountain View','Hood River','Cooper Mountain',\n",
    "               'Highland Park','Columbia River','Meridian Park','Merlo Park','Space Mountain','ATandT Park','Maywood Park','Fenway Park',\n",
    "               'Coors Field']\n",
    "\n",
    "# tweets in NYC that have reigonally unique context\n",
    "keyWordsNYC = ['Sunset Park','Belmont Park','Forest Hills','Garden City','the Garden','Borough Park',\n",
    "               'Mad Square Garden','Madsquaregarden', 'square garden','Massapequa Park','Citi Field',\n",
    "               'Lone Star Park','Deer park','Pearl River', 'Kings Park','Floral Park','Ozone Park',\n",
    "               'rego park', 'rockaway park','Hyde Park','Fenway Park','PNC Park','Marine Park','Harlem River',\n",
    "               'Hutchinson River','Lake George','Park Slope','Massapequa Park','Bronx River','Nats Park',\n",
    "               'Mountain View','Nationals Park','Salt Lake','Madison Sq','Florham Park','Morris Park',\n",
    "               'Forest Park','Finsbury Park','Hershey Park','Silver Lake' ]\n",
    "\n",
    "padKey = \"AAAAAAAAAAAAAAAAAA \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled preprocessed data\n",
    "def loadDatasets(pickleParams):\n",
    "    trainDict = pickle.load(open(pickleParams['trainDictPicklePath'],'rb'))\n",
    "    devDict = pickle.load(open(pickleParams['devDictPicklePath'],'rb'))\n",
    "    testDict = pickle.load(open(pickleParams['testDictPicklePath'],'rb'))\n",
    "    NYC_Dict = pickle.load(open(pickleParams['NYC_DictPicklePath'],'rb'))\n",
    "    embeddingMatrix = pickle.load(open(pickleParams['embeddingMatrixPicklePath'],'rb'))\n",
    "    word2IndexMap = pickle.load(open(pickleParams['word2IndexPicklePath'],'rb'))\n",
    "    return(trainDict,devDict,testDict,NYC_Dict,embeddingMatrix,word2IndexMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test whether substring variable is a subset of the wholestring variable.  Used in a map-reduce function\n",
    "def testForSubstring(wholeString,subString):\n",
    "    if(subString.lower() in wholeString):\n",
    "        return subString\n",
    "    return(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace instances of keyWords in the wholeString with the padKey string\n",
    "def replaceKeywords(wholeString,keyWords,padKey):\n",
    "    newString = wholeString.lower()\n",
    "    for word in keyWords:\n",
    "        if word.lower() in newString:\n",
    "            wordLength = len(word.split())\n",
    "            padding = padKey*wordLength\n",
    "            startIndex = newString.find(word.lower())\n",
    "            endIndex = newString.find(\" \",startIndex+len(word))\n",
    "            stringToReplace = newString[startIndex:endIndex]\n",
    "            newString = newString.replace(stringToReplace,padding[:-1])\n",
    "    return(newString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareWords(padKey,compareWord):\n",
    "    if(padKey == compareWord):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify regional indicators for one tweet ###\n",
    "**Inputs**: <br>\n",
    "- **wholeString** (string) - text of single tweet to screen for regional indicators <br>\n",
    "- **keyWords** (string array) - phrases which if present within wholeString should be flagged with a positive indicator variable <br>\n",
    "- **padKey** (string) - intermediate used to identify where within a tweet regional phrases are present <br>\n",
    "**Outputs**: <br>\n",
    "- **loc_indicator** (int array) - series of regional indicators, where each index flags whether the word at index 1 belongs to a regional key phrase or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add location indicator to a single \n",
    "def createSingleLocInd(wholeString,keyWords,padKey):\n",
    "    codedString = replaceKeywords(wholeString,keyWords,padKey)\n",
    "    parsedCode = codedString.split()\n",
    "    padKeyArray = [padKey[:-1]]*len(parsedCode)\n",
    "    locResults = list(map(compareWords,parsedCode,padKeyArray))\n",
    "    loc_indicator = np.asarray(locResults)\n",
    "    return(loc_indicator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create regional indicators for all tweets in a single dataset ###\n",
    "**Inputs**: <br>\n",
    "- **textArray** (string array) - all texts in the dataset <br>\n",
    "- **keyWords** (string array) - key phrases that are regional indciators.  Indices of the words in these phrases are where regional indicator flags should be positive <br>\n",
    "- **padKey** (string) - intermediate to communicate where regional indicators where indentified within a text <br>\n",
    "\n",
    "**Outputs**:\n",
    "- **padResults** (array of int arrays) - the ith int array corresponds to the reiongal indicator flags for all words in the ith text string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createAllLocInd(textArray,keyWords,padKey):\n",
    "    keyWordMap = [keyWords]*len(textArray)\n",
    "    padKeyMap = [padKey]*len(textArray)\n",
    "    padResults = list(map(createSingleLocInd,textArray,keyWordMap,padKeyMap))\n",
    "    return(padResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add regional indicator to multiple datasets ### \n",
    "**Inputs**: <br>\n",
    "- **savePaths** (string array) - absolute filepaths where modified datasets should be stored <br>\n",
    "- **dictArray** (dict array) - input dictionaries to add regional indicator variable to <br>\n",
    "- **keyWords** (string array) - keyStrings that are positive indicators <br>\n",
    "- **padKey** (string) - intermediate product - used to identify where in a text keyWords occur and positive regional indicators should be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add regional location indicator to datasets \n",
    "def addLocToAllDatasets(savePaths,dictArray,keyWords,padKey):\n",
    "\n",
    "    #testString = \"I'm testing to see if Fire on the mountain and merlo Park are picked up by the map function\"\n",
    "\n",
    "    index=0\n",
    "    for tempDict in dictArray:\n",
    "        tempDictLocInd = createAllLocInd(tempDict['sent'],keyWords,padKey)\n",
    "        tempDict['loc_ind'] = tempDictLocInd\n",
    "        pickle.dump(tempDict,open(savePaths[index],\"wb\" ))\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['x', 'y', 'seqlens', 'hash', 'emot', 'loc_ind', 'sent', 'labels', 'seqLens'])\n",
      "dict_keys(['x', 'y', 'seqlens', 'hash', 'emot', 'loc_ind', 'sent', 'labels', 'seqLens'])\n",
      "dict_keys(['x', 'y', 'seqlens', 'hash', 'emot', 'loc_ind', 'sent', 'labels', 'seqLens'])\n",
      "dict_keys(['labels', 'seqLens', 'sent', 'hash', 'emot', 'loc_ind'])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    trainDict, devDict, testDict, NYC_Dict,embeddingMatrix, word2IndexMap = loadDatasets(datasetPickleParams)\n",
    "    PDXDicts = [trainDict,devDict,testDict]\n",
    "    PDXSavePaths = [\n",
    "        parentFolder + 'trainDictv2t.p',\n",
    "        parentFolder + 'devDictv2t.p',\n",
    "        parentFolder + 'testDictv2t.p'\n",
    "    ]\n",
    "    addLocToAllDatasets(PDXSavePaths,PDXDicts,keyWordsPDX,padKey)\n",
    "    addLocToAllDatasets([parentFolder + 'NYCDictv2t.p'],[NYC_Dict],keyWordsNYC,padKey)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
