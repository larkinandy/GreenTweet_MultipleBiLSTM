{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for preprocessing tweets for bi-LSTM training #\n",
    "\n",
    "**Author:** [Andrew Larkin](https://www.linkedin.com/in/andrew-larkin-525ba3b5/) <br>\n",
    "**Affiliation:** [Oregon State University, College of Public Health and Human Sciences](https://health.oregonstate.edu/) <br>\n",
    "**Date Created:** October 14th, 2018 <br>\n",
    "\n",
    "**Summary** <br>\n",
    "This script contains functions used to preprocess tweets and corresponding metadata as features for deep learning model input preprocessing includes:\n",
    "\n",
    "- tokenization\n",
    "- partitioning multiple word hashtags (e.g. #lastchildinthewoods) into multiple words and adjusting hashtag indicators and sentence length metadata\n",
    "- encoding words to 300 length word vectors created by Stanford \n",
    "- creating trainable, randomly initizlied word encodings for unknown and pad token words\n",
    "- replace uknown words with an UNKNOWN tag\n",
    "- fill the end sentences with empty tag so all tweet vectors are the same length\n",
    "- apply QA/QC inclusion criteria\n",
    "- create train, dev, and test datasets\n",
    "- calculate descriptive statistics\n",
    "\n",
    "**Note**: text was already partially processed at the time of record insertion into the SQL database.  Input text has already had emojis replaced with word descriptions, hashtag and emoji indicators created, and hyperlinks removed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup ###\n",
    "\n",
    "** Import libraries, define model and dataset parameters, and set filepaths **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "import re, string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filepaths\n",
    "parentFolder =  \"C:/Users/larkinan/Desktop/DBTraining/\"\n",
    "outputFolder = parentFolder + \"preprocessingOutput/\" # folder to store train, dev, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters and constants for the deep learning model.  \n",
    "# While only the word_vec_dim is needed for this script, \n",
    "# the entire dictionary is copied here to facilitate ease of search and comparison.\n",
    "modelParams = {\n",
    "    'word_vec_dim':300, # dimension of each word vector\n",
    "    'mini_batch_size':256, \n",
    "    'learning_rate':0.01,\n",
    "    'momentum':0.9,\n",
    "    'num_outcomes':1, # whether testing for just 1 or multiple outcomes,\n",
    "    'hidden_layer size':32,\n",
    "    'num_epochs':10000,\n",
    "    'num_dev':5000,\n",
    "    'num_test':5000,\n",
    "    'hidden_layer_activation':'tanh'\n",
    "}\n",
    "\n",
    "twitterCSVParams = {\n",
    "    'filepath':parentFolder + \"trainingDatav8.csv\",\n",
    "    'classifyFilepath':parentFolder + \"allScores__Oct14_18.csv\",\n",
    "    'text':'clean_text',  # whether to use cleaned or raw (i.e. with emojis, hashtags, etc) twitter text\n",
    "    'nature_ind':'nature',\n",
    "    'safety_ind':'safety',\n",
    "    'beauty_ind':'beauty',\n",
    "    'exercise_ind':'exercise',\n",
    "    'social_ind':'social',\n",
    "    'stress_ind':'stress',\n",
    "    'air_ind':'air',\n",
    "    'hashtag_ind':'hash_ind',\n",
    "    'emoticon_ind':'emot_ind' \n",
    "}\n",
    "\n",
    "wordVecParams = {  # word encodings created by Stanford\n",
    "    'zipFilename':parentFolder + \"glove.840B.300d.zip\",\n",
    "    'txtFilename':\"glove.840B.300d.txt\"\n",
    "}\n",
    "\n",
    "unknownWordSubs = { # identified using the 'countUnknownWords' function\n",
    "    \"'04\":'4',\n",
    "    \"'10\":'10',\n",
    "    \"'12\":'12',\n",
    "    \"'14\":'14',\n",
    "    \"'15\":'15',\n",
    "    \"'16\":'16',\n",
    "    \"'17\":'17',\n",
    "    \"'18\":'18',\n",
    "    \"'72\":'72',\n",
    "    \"'93\":'93',\n",
    "    \"'96\":'96',\n",
    "    \"'cause\":'because',\n",
    "    \"'ll\":'will',\n",
    "    \"'m\":'am',\n",
    "    \"'re\":'are',\n",
    "    \"'s\":'is',\n",
    "    \"'ve\":'have',\n",
    "    \"'Ve\":'have',\n",
    "    \"'em\":'them'\n",
    "}\n",
    "\n",
    "picleParams = { # where to store datasets for model training on hard disk\n",
    "    \"trainDictPicklePath\":outputFolder + \"trainDict.p\",\n",
    "    \"devDictPicklePath\":outputFolder + \"devDict.p\",\n",
    "    \"testDictPicklePath\":outputFolder + \"testDict.p\",\n",
    "    \"allDictPicklePath\":outputFolder + \"allDict.p\",\n",
    "    \"embeddingMatrixPicklePath\":outputFolder + \"embeddingMatrix.p\",\n",
    "    \"word2IndexPicklePath\":outputFolder + \"word2Index.p\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert binary variable to two binary variables ###\n",
    "Classification in Tensorflow requires a unique vector for each classification level.  This function converts a single binary classification vector into two binary classification vectors, one vector for positive classification (value = 1 in the original input vector) and one vector for negative classification (value = 0 in the original input vector \n",
    "\n",
    "**TODO: <br> **\n",
    "[  ] consider optimizing by converting for loop into vector operation.  But, it runs very fast even with a loop.  Maybe python is unrolling the loop?\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inputLabels ** (n x 1 numpy matrix) - Values are either 1 for positive classification and 0 for negative classification.  \n",
    "- **debug** (boolean) - whether or not to print debug statements for testing/validation\n",
    "\n",
    "**Outputs:** <br>\n",
    "- **twoClassLabels** (n x 2 numpy matrix).  First column values are 1 for positive classification, 0 otherwise.  Second column values are 1 for negative classification, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expandOneClassLabel(inputLabels,debug=False):\n",
    "    assert(max(inputLabels) == 1)\n",
    "    assert(min(inputLabels) == 0)\n",
    "    positiveClass = np.reshape(np.array(inputLabels),(len(inputLabels),1))\n",
    "    negativeClass = np.ones((len(inputLabels),1))\n",
    "    negativeClass = np.subtract(negativeClass, positiveClass)\n",
    "    twoClassLabels = np.concatenate((positiveClass,negativeClass),axis=1)\n",
    "    if(debug):\n",
    "        print(\n",
    "            \"epandOneClassLabel - input dim = %s, output dim = %s\" \n",
    "            % (positiveClass.shape, twoClassLabels.shape)\n",
    "        )\n",
    "    assert(sum(np.sum(twoClassLabels,axis=1)) == len(inputLabels))\n",
    "    return(twoClassLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sumInputs(inputLabels):\n",
    "    return sum(inputLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming class labels for Tensorflow compatibility ###\n",
    "Tensorflow requires one vector for each potential class. This function transforms a binary vector of class labels into two numpy arrays, one for positive classification and one for negative classification.  Finally, arrays for all 7 outcomes are combined to create a single nx14 numpy matrix. \n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inputDict** (dict) - dict containing class label arrays, with one array for each outcome of interest <br>\n",
    "- **twitterParamDict** (dict) - contains names for each class label <br>\n",
    "- **debug** (boolean) - whether ot not to print debug statements\n",
    "\n",
    "**Outputs** <br>\n",
    "- **LabelMatrix** (numpy matrix) - n x14 numpy matrix of expanded outcomes, with outcomes listed in the same column order as inputs.\n",
    "\n",
    "\n",
    "![Alt text](https://raw.githubusercontent.com/larkinandy/GreenTweet_MultivariateBiLSTM/master/DataPreprocessing/images/methods_images.jpg)\n",
    ">> **Figure 1.** Diagram depicting the input and output of expandAllClassLabels function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandAllClassLabels(inputDict,twitterParamDict,debug=False):\n",
    "    inputLabels = getClassLabelsFromDict(inputDict,twitterParamDict)\n",
    "    sumClassifiers = list(map(sumInputs,inputLabels))\n",
    "    tempLabels = list(map(expandOneClassLabel,inputLabels))\n",
    "    outLabels = np.concatenate(tempLabels,axis=1).astype(np.int32)\n",
    "    if(debug):\n",
    "        print(\"sample input Labels before expanding labels: %s\" % str(inputLabels[1][0:10]))\n",
    "        print(\"sample temp labels after expanding labels: %s\" % str(inputLabels[1][0:10]))\n",
    "        print(\"shape of output matrix: %s\" % str(outLabels.shape))\n",
    "    # verify num positive records for nature class in output is same as input\n",
    "    assert(np.sum(outLabels,axis=0)[0] == sumClassifiers[0]) \n",
    "    assert(outLabels.shape[0] == len(inputLabels[0]) and outLabels.shape[1] == len(inputLabels)*2)\n",
    "    return(outLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove records that do not meet inclusion criteria###\n",
    "Some records have too little context or are too noisy to accurately classify.  This step removes records with too few words (both including and excluding unknown words) and too high a percentage of unknown words.  This is to reduce the bias in the model and develop a better understanding of the bayes optimal error for the target classifiers.\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inputData** (dict) - contains sentences to be screened along with len of sentences.  At this point sentences may already be padded with end tokens, so we can't simply use the len function to calculate sentence length <br>\n",
    "- **minLength** (int) - minimum number of words (including unknowns) for inclusion in the dataset <br>\n",
    "- **minnumVals** (int) - mimimum number of words (excluding unknowns) for inclusion in the datastet <br>\n",
    "- **maxPercUnk** (float) - max percentage of words that can be unknown for inclusion in the dataset <br>\n",
    "\n",
    "**Outputs** <br>\n",
    "- **scrDict** (dict) - revised version of inputData, with sentences removed that failed to meet inclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyExclusionCriteria(inputData,minLength =5,minNumVals=3,maxPercUnk=0.25):\n",
    "    numValid, index = (0 for i in range(2))\n",
    "    scrSents, scrLengs, scrLabels, scrHash, scrEmot = ([] for i in range(5))\n",
    "    for sent in inputData['paddedSents']:\n",
    "        splitVals = sent.split(\" \")\n",
    "        b = inputData['sentLengths'][index]\n",
    "        splitVals = splitVals[0:b]\n",
    "        numWords = len(splitVals)\n",
    "        numUnk = sum(word == 'UNK' for word in splitVals)\n",
    "        percUnk = (numUnk*1.0)/(numWords*1.0)\n",
    "        if(numWords > minLength and (numWords - numUnk) > minNumVals and percUnk < maxPercUnk):\n",
    "            numValid+=1\n",
    "            scrSents.append(inputData['paddedSents'][index])\n",
    "            scrLengs.append(inputData['sentLengths'][index])\n",
    "            scrLabels.append(inputData['twoClassLabels'][index])\n",
    "            scrHash.append(inputData['paddedHashtags'][index])\n",
    "            scrEmot.append(inputData['paddedEmots'][index])\n",
    "        index +=1\n",
    "    scrDict = {'sent':scrSents,'seqLens':scrLengs,'labels':scrLabels,'hash':scrHash,'emot':scrEmot}\n",
    "    return(scrDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary from multiple arrays based on a set of indices\n",
    "def getDataDict(indices,data_x,data_y,data_seqlens,data_hash,data_emot,labels):\n",
    "    subset_x = [data_x[i] for i in indices]\n",
    "    subset_y = np.asarray([data_y[i] for i in indices]).reshape((len(indices), 14))\n",
    "    subset_seqlens = [data_seqlens[i] for i in indices]\n",
    "    subset_hash = [data_hash[i] for i in indices]\n",
    "    subset_emot = [data_emot[i] for i in indices]\n",
    "    dataDict = {labels[0]:subset_x,labels[1]:subset_y,labels[2]:subset_seqlens,labels[3]:subset_hash,labels[4]:subset_emot}\n",
    "    return(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partition datasets into train, dev, and test subsets and store in dict format\n",
    "def splitTrainDevTest(modelParams,screenedDict):\n",
    "    \n",
    "    numDev = modelParams['num_dev']\n",
    "    numTest = modelParams['num_test']\n",
    "    data_x = screenedDict['sent']\n",
    "    data_y = screenedDict['labels']\n",
    "    data_seqlens = screenedDict['seqLens']\n",
    "    hashInd = screenedDict['hash']\n",
    "    emotInd = screenedDict['emot']\n",
    "    \n",
    "#def splitTrainDevTest(numDev,numTest,data_x,data_y,data_seqlens,hashInd,emotInd):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    devBatch = instance_indices[0:numDev]\n",
    "    devDict = getDataDict(devBatch,data_x,data_y,data_seqlens,hashInd,emotInd,\n",
    "               ['x','y','seqlens','hash','emot'])\n",
    "    testBatch = instance_indices[numDev:numDev+numTest]\n",
    "    testDict = getDataDict(testBatch,data_x,data_y,data_seqlens,hashInd,emotInd,\n",
    "               ['x','y','seqlens','hash','emot'])\n",
    "    trainBatch = instance_indices[numDev+numTest:]\n",
    "    trainDict = getDataDict(trainBatch,data_x,data_y,data_seqlens,hashInd,emotInd,\n",
    "               ['x','y','seqlens','hash','emot'])\n",
    "    allDict = getDataDict(instance_indices,data_x,data_y,data_seqlens,hashInd,emotInd,\n",
    "                          ['x','y','seqlens','hash','emot'])\n",
    "    assert(len(devDict['x']) == numDev)\n",
    "    assert(len(testDict['x'])==numTest)\n",
    "    assert(len(trainDict['x'])==(len(allDict['x'])-(numDev+numTest)))\n",
    "    return devDict,testDict,trainDict,allDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClassLabelsFromDict(twitterData,twitterParamDict):\n",
    "    natureInd = twitterData[twitterCSVParams['nature_ind']]\n",
    "    safetyInd = twitterData[twitterCSVParams['safety_ind']]\n",
    "    beautyInd = twitterData[twitterCSVParams['beauty_ind']]\n",
    "    exerciseInd = twitterData[twitterCSVParams['exercise_ind']]\n",
    "    socialInd = twitterData[twitterCSVParams['social_ind']]\n",
    "    stressInd = twitterData[twitterCSVParams['stress_ind']]\n",
    "    airInd = twitterData[twitterCSVParams['air_ind']]\n",
    "    return(natureInd,safetyInd,beautyInd,exerciseInd,socialInd,stressInd,airInd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create mapping dictionaries for all words in the twitter dataset ###\n",
    "The mapping dictionaries are imporrtant for converting words into vectors and vice versa in later functions. Note that dictionaries include punctuation and  \n",
    "\n",
    "**Inputs: ** <br>\n",
    "- **inputSents ** (list) - twitter sentences containing all candidate words to map to\n",
    "\n",
    "**Outputs: ** <br>\n",
    "- ** index2word_map ** (dict) - dictionary with index as the key and word as the value\n",
    "- ** word2index_map ** (dict) - dictionary with words as the key and index as the value\n",
    "\n",
    "** TODO: ** consider vectorizing, but it runs very fast even with nested loops (< 1s for 100,000 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateWordMap(inputSents):\n",
    "    word2index_map = {}\n",
    "    index = 0\n",
    "    for sent in inputSents:\n",
    "        reducedPunc = string.punctuation.replace(\"'\", \" \")\n",
    "        for word in sent:\n",
    "            if(len(word)>0 and word[0] == \"'\"):                \n",
    "                word = word[1:]\n",
    "            if(len(word)>0 and word[-1] == \"'\"):\n",
    "                word = word[:-1]\n",
    "            if(len(word)>0 and word not in word2index_map):\n",
    "                    word2index_map[word] = index\n",
    "                    index+=1\n",
    "    index2word_map = {word:index for word, index in word2index_map.items()}\n",
    "    return index2word_map, word2index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load class scores and twitter data from csv files \n",
    "def loadCSV(inputCSVFile,debug=False,inDelim=',',inQuoteChar='\"'):\n",
    "    rawData = ps.read_csv(inputCSVFile,encoding='utf-8',delimiter = inDelim,quotechar = inQuoteChar)\n",
    "    if(debug):\n",
    "        print(rawData.head())\n",
    "    rawData.tweet_id = rawData.tweet_id.astype(str)\n",
    "    return(rawData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract csv filepaths from param dictionary \n",
    "def getTwitterCSVFilepaths(twitterDict):\n",
    "    CSVFilepath = twitterCSVParams['filepath']\n",
    "    classifyFilepath = twitterCSVParams['classifyFilepath']\n",
    "    return CSVFilepath, classifyFilepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load twitter data features \n",
    "# Get the names of features and class labels from the twitter params dictionary\n",
    "def getTwitterCSVParams(twitterDict):\n",
    "    CSVFilepath = twitterCSVParams['filepath']\n",
    "    classifyFilepath = twitterCSVParams['classifyFilepath']\n",
    "    textVar = twitterCSVParams['text']\n",
    "    hashtagVar = twitterCSVParams['hashtag_ind']\n",
    "    emoticonVar = twitterCSVParams['emoticon_ind']\n",
    "    natureInd = twitterCSVParams['nature_ind']\n",
    "    safetyInd = twitterCSVParams['safety_ind']\n",
    "    beautyInd = twitterCSVParams['beauty_ind']\n",
    "    exerciseInd = twitterCSVParams['exercise_ind']\n",
    "    socialInd = twitterCSVParams['social_ind']\n",
    "    stressInd = twitterCSVParams['stress_ind']\n",
    "    return (\n",
    "        [CSVFilepath,classifyFilepath, textVar, hashtagVar, emoticonVar],\n",
    "        [natureInd,safetyInd,beautyInd,exerciseInd,socialInd,stressInd]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load class labels and twitter data from csv files and merge ###\n",
    "\n",
    "**Inputs** <br>\n",
    "- **twitterCSVParams** (dict) - contains column names and filepaths.  See Setup section above for more details\n",
    "- **debug** (boolean) - whether to print debug statements in subfunctions\n",
    "\n",
    "**Outputs** <br>\n",
    "- **tweetWithScores ** (dict) - contains class labels and twitter data merged into single dictionary.  Dictionary params are documented in the setup section above\n",
    "\n",
    "**TODO** <br>\n",
    "[ ] - combine csv files and simplify the data loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadWordList(twitterCSVParams,debug=False):\n",
    "    TwitterFilepath, classifyFilepath = getTwitterCSVFilepaths(twitterCSVParams)\n",
    "    classScores = loadCSV(classifyFilepath,debug)\n",
    "    assert(max(classScores.drop('tweet_id', 1).max())==1 and min(classScores.drop('tweet_id', 1).min()) == 0)\n",
    "    tweetData = loadCSV(TwitterFilepath,debug,inQuoteChar='|')\n",
    "    tweetWithScores = tweetData.merge(classScores,  how='inner', left_on='tweet_id', right_on = 'tweet_id')\n",
    "    return tweetWithScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the word vectors ###\n",
    "Read word vectors from file and load into memory.  Note that this file is large and compressed.  Recommend reading from an SSD drive and a computer with at least 16GB of memory (run on intel 900p storage with with 32GB RAM).\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **vectorFilepath** (string) - entire filepath to zip file containing word vectors \n",
    "\n",
    "**Outputs:** <br>\n",
    "- **embeddingWeights** (dict) - dictionary, where words are the keys and vectors are the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadWordVec(wordVecParams):\n",
    "    embedding_weights = {}\n",
    "    test = 0\n",
    "    with zipfile.ZipFile(wordVecParams['zipFilename']) as z:\n",
    "        with z.open(wordVecParams['txtFilename']) as f:\n",
    "            for line in f:\n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\"))\n",
    "                coefs = np.asarray(vals[1:], dtype = 'float32')\n",
    "                coefs/=np.linalg.norm(coefs)\n",
    "                embedding_weights[word] = coefs\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the longest word at the end of a string of characters.  Return -1 if no word exists###\n",
    "this function is for partitioning multiple words in tweets, compacted together without a space (e.g. lastchildinthewoods)\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **wordSpace** (string) - string that potentially contains multiple words concatenated without spaces\n",
    "- **inDict** (dictionary) - the word search space, i.e. list of all words we consider to be 'valid'\n",
    "- **debug** (boolean) - whether to print debug statements\n",
    "\n",
    "**Outputs:** <br>\n",
    "- integer where string can be partitioned to subset words from wordSpace.  -1 if no such partition exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchForFirstWord(wordSpace,inDict,debug=False):\n",
    "    lastWordLen = len(wordSpace)\n",
    "    while(lastWordLen > 0):\n",
    "        candidateWord = wordSpace[0:lastWordLen]\n",
    "        if(candidateWord in inDict):\n",
    "            endInd = lastWordLen\n",
    "            if(debug):\n",
    "                print(\n",
    "                    \"found long word in wordspace: \\n word: %s \\n index: %i\" \n",
    "                    % (candidateWord,endInd)\n",
    "                )\n",
    "            return endInd\n",
    "        lastWordLen -=1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for a single string of an unknown word, partition the string into smaller words ###\n",
    "This function also extends the hashtag and emoticon indicator vectors corresponding to the increase in word length\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inWord** (string) - word to partition\n",
    "- **inDict** (dict) - contains all acceptable or 'legal' words\n",
    "- **isHashtag** (int array) - binary vector indicating whether each word in a sentence is from  a hashtag. Update to reflect new words\n",
    "- **isEmot** (int array) - binary vector indicating whether each word in a sentence if from an emoticon.  Update to reflect new words\n",
    "- **debug** (boolean) - whether to print debug statements \n",
    "\n",
    "**Outputs:** <br>\n",
    "- **wordVec** (string array) - list of words, in sequential order of original sentence\n",
    "- **outHashtag** (int array) - binary hashtag vector, updated to reflect any added words\n",
    "- **outEmot** (int array) - binary emot vector, updated to reflect any added words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkOneCompactedWord(inWord,inDict,isHashtag,isEmot,debug=False):\n",
    "    wordVec, outHashtag, outEmot = ([] for i in range(3))\n",
    "    origWord = inWord\n",
    "    partitionIndex = searchForFirstWord(inWord,inDict,debug)\n",
    "    while(partitionIndex >0):\n",
    "        outHashtag.append(isHashtag)\n",
    "        outEmot.append(isEmot)\n",
    "        wordVec.append(inWord[0:partitionIndex])\n",
    "        inWord = inWord[partitionIndex:]\n",
    "        partitionIndex = searchForFirstWord(inWord,inDict,debug)\n",
    "    if(len(inWord)>0):\n",
    "        wordVec.append(inWord)\n",
    "        outHashtag.append(isHashtag)\n",
    "        outEmot.append(isEmot)\n",
    "    if(len(wordVec)==0):\n",
    "        wordVec = [origWord]\n",
    "        outHashtag = isHashtag\n",
    "    if(debug):\n",
    "        print(\n",
    "            \"%s was partitioned into %i words: %s \\n with hashtag: %s\"\n",
    "            % (origWord,len(wordVec),wordVec,outHashtag)\n",
    "             )\n",
    "    assert (len(wordVec) == len(outHashtag) and len(outHashtag) == len(outEmot))\n",
    "    return(wordVec,outHashtag,outEmot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for a single sentence, split the sentence into individual words, and split compacted words that are missing a space ###\n",
    "\n",
    "this function also adjusts hashtag and emoji indicator vectors to accomadate expanded list of words\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inSent** (string) - entire sentence <br>\n",
    "- **inDict** (dict) - dictionary of allowed, or 'legal' words <br>\n",
    "- **inHashtag** (int array) - indicator of which words in the sentence, if any, belong to a hashtag <br>\n",
    "- **inEmot** (int array) - indicator of which words in the sentence, if any, belong to an emoji description <br>\n",
    "- **debug** (boolean) - whether debug statements should be printed <br>\n",
    "\n",
    "**Outputs:** <br>\n",
    "- **splitSent** (string array) - revised sentence in array format, with each word an element in the array <br>\n",
    "- **outHashtag** (int array) - revised hashtag indicator array <br>\n",
    "- **outEmot** (int array) - revised emot indicator array <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def screenForCompactedWordsInSentence(inSent,inDict,inHashtag,inEmot,debug=False):\n",
    "    splitSent, outHashtag, outEmot = ([] for i in range(3))\n",
    "    wordIndex = 0\n",
    "    for word in inSent.split(\" \"):\n",
    "        if(len(word)>0):\n",
    "            try:\n",
    "                currHashtag = inHashtag[wordIndex]\n",
    "                currEmot = inEmot[wordIndex]\n",
    "            except Exception as e:\n",
    "                if(debug):\n",
    "                    print(str(e))\n",
    "            if not word in inDict:\n",
    "                splitWords,hashtags,emotes = checkOneCompactedWord(word,inDict,currHashtag,currEmot,debug)\n",
    "                splitSent += splitWords\n",
    "                outHashtag += hashtags \n",
    "                outEmot += emotes\n",
    "            else:\n",
    "                splitSent.append(word)\n",
    "                outHashtag.append(currHashtag)\n",
    "                outEmot.append(currEmot)\n",
    "            if(word not in string.punctuation and word not in ['...']):\n",
    "                wordIndex +=1\n",
    "    if(debug):\n",
    "        print(\n",
    "            \"sentence after splitting: %s \\n hashtag: %s \\n emote: %s\"\n",
    "            % (splitSent,outHashtag,outEmot)\n",
    "        )\n",
    "    assert(len(splitSent) == len(outHashtag) & len(outHashtag) == len(outEmot))\n",
    "    return splitSent, outHashtag, outEmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for all sentences in the dataset, split the sentences into individual words, and split compacted words that are missing a space ###\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **TweetSentences** (string array) - sentences to process <br>\n",
    "- **inDict** (dict) - dictionary of allowable or 'legal' words <br>\n",
    "- **inHashtag** (list of integer arrays) - one integer array for each sentence, indicating which words belong to a hashtag <br>\n",
    "- **inEmot** (list of integer arrays) - one integer array for each sentence, indicating which words belong to an emoticon description <br>\n",
    "- **debug** (boolean) - whether to print debug statements <br>\n",
    "\n",
    "**Outputs:**<br>\n",
    "- **screenedSents** (list of string arrays) - revised version of Tweet sentences, with compacted words partitioned into multiple words.  Each sentence is now an arrays of strings instead of a single string <br>\n",
    "- **screenedHashtags** (list of integer arrays) - revised version of inHashtags, adjusted for word insertions <br>\n",
    "- **screenedEmots** (list of integer arrays) - revised version of inEmot, adjusted for word insertions <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def screenForCompactedWordsAllTweets(TweetSentences,inDict,inHashtag,inEmot,debug=False):\n",
    "    screenedSents, screenedHashtags, screenedEmots = ([] for i in range(3))\n",
    "    screenedHashtags = []\n",
    "    screenedEmots = []\n",
    "    for currIndex in range(len(TweetSentences)):\n",
    "        splitSent, updatedHashtag, updatedEmot = screenForCompactedWordsInSentence(\n",
    "            TweetSentences[currIndex],\n",
    "            inDict,\n",
    "            inHashtag[currIndex],\n",
    "            inEmot[currIndex],\n",
    "            debug = False\n",
    "        )\n",
    "        screenedSents.append(splitSent)\n",
    "        screenedHashtags.append(updatedHashtag)\n",
    "        screenedEmots.append(updatedEmot)\n",
    "    if(debug):\n",
    "        print(\n",
    "            \"Example screening for compact words: \\n %s, %s, %s\"\n",
    "            % (screenedSents[0],screenedHashtags[0],screenedEmots[0])\n",
    "        )\n",
    "        print(\n",
    "            \"Number of tweets screened: %i\" %len(screenedSents)\n",
    "        )\n",
    "    assert(len(screenedSents) == len(screenedHashtags) & len(screenedHashtags) == len(screenedEmots))\n",
    "    return(screenedSents,screenedHashtags,screenedEmots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove word encodings whose corresponding word isn't in the Twiter dataset\n",
    "def reduceWordEmbeddings(embeddedWeights,word2indexMap):\n",
    "    reducedEmbeddingWeights = {}\n",
    "    for key, value in embeddedWeights.items():\n",
    "        if key in word2indexMap:\n",
    "            reducedEmbeddingWeights[key] = embeddedWeights[key]\n",
    "    return reducedEmbeddingWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace a single word not found in the dictionary with an unknown token\n",
    "def tagSingleWord(word,unknownTag,word2indexMap,subWords):\n",
    "    if(len(word)>0):\n",
    "        if word not in word2indexMap:\n",
    "            if word in subWords:\n",
    "                word = subWords[word]\n",
    "            else:\n",
    "                word = unknownTag\n",
    "        return(word)\n",
    "    return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all sentences, replace all words not found in the dictionary with an unknown token\n",
    "def substituteAllUnknownTags(inputSentences,unknownTag,word2index_map,subWords):\n",
    "    taggedSentences = []\n",
    "    for sent in inputSentences:\n",
    "        screenedSentence = \"\"  \n",
    "        for word in sent:\n",
    "            taggedWord = tagSingleWord(word,unknownTag,word2index_map,subWords)\n",
    "            if not taggedWord == None: screenedSentence += taggedWord + \" \"\n",
    "        screenedSentence = screenedSentence[:-1]\n",
    "        taggedSentences.append(screenedSentence)\n",
    "    return(taggedSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map word encoding to corresponding word, and create a lookup dictionary.\n",
    "def convertVectorsToMatrix(word2embeddingDict,vecDim):\n",
    "    embeddingMatrix = np.zeros((len(word2embeddingDict)+2,vecDim))\n",
    "    wordToIndex = {}\n",
    "    index = 0\n",
    "    for word, vector in word2embeddingDict.items():\n",
    "        wordEmbedding = word2embeddingDict[word]\n",
    "        embeddingMatrix[index,:] = wordEmbedding\n",
    "        wordToIndex[word] = index\n",
    "        index +=1\n",
    "    return(embeddingMatrix,wordToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify unknown words and their corresponding frequency\n",
    "def countUnknownWords(embeddingWeights,screenedSents):\n",
    "    unknownWordDict = {}\n",
    "    for sent in screenedSents:\n",
    "        for word in sent:\n",
    "            if word not in embeddingWeights:\n",
    "                if word in unknownWordDict:\n",
    "                    unknownWordDict[word] = unknownWordDict[word] + 1\n",
    "                else:\n",
    "                    unknownWordDict[word] = 1\n",
    "    return(unknownWordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace unknown words with list of known substitutions\n",
    "# for example, replace \"'ve\" with \"have\"\n",
    "def substituteUnknownWords(screenedSents,unknownWordDict):\n",
    "    for sentIndex in range(len(screenedSents)):\n",
    "        for wordIndex in range(len(screenedSents[sentIndex])):\n",
    "            if screenedSents[sentIndex][wordIndex] in unknownWordDict:\n",
    "                screenedSents[sentIndex][wordIndex] = unknownWordDict[screenedSents[sentIndex][wordIndex]]\n",
    "    return screenedSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the number of words in the longest tweet (by word count) in the database ###\n",
    "To do this we find the max length of the emoticon indicator vectors, since each vector contains one element for one word in a sentence\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inputEmoticons** (list of integer arrays) - one integer array for each sentence.  Len of array corresponds to number of words in the sentence <br>\n",
    "\n",
    "**Outputs:** <br>\n",
    "- **maxSentenceLength** (int) - number of words in the longest tweet (by word count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findMaxSentenceLength(inputEmoticons):\n",
    "    maxSentenceLength = 0\n",
    "    for emot in inputEmoticons:\n",
    "        maxSentenceLength = max(len(emot), maxSentenceLength)\n",
    "    maxSentenceLength +=1\n",
    "    return(maxSentenceLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad shorter sentences with pad tokens to ensure all sentences have the same vector length ###\n",
    "this allows data to be stored in matrices, presenting the opportunity to vectorize deep learning and perform minibatch rather than stochastic gradient descent training \n",
    "\n",
    "**Inputs:**<br>\n",
    "- **inputSentences** (list of string arrays) - sentences that need to be padded <br>\n",
    "- **maxSentenceLength** (int) - number of words that each sentence must contain.  Add pad tokens to the end of shorter sentences to increase their length <br>\n",
    "- **padToken** (string) - specific word to add to sentences to indicate end of sentence has already been reached <br>\n",
    "\n",
    "**Outputs:**<br>\n",
    "- **paddedSents** (list of string arrays) - revised version of inputSentences, with padding added so all sentences have maxSentenceLength number of words <br>\n",
    "- **sentLengths** (integer array) - number of words in each sentence **before** adding padding <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padSentences(inputSentences,maxSentenceLength,padToken):\n",
    "    paddedSents = []\n",
    "    sentLengths = []\n",
    "    for sent in inputSentences:\n",
    "        sentLength = len(sent.split(\" \"))\n",
    "        sentLengths.append(sentLength)\n",
    "        while(sentLength < maxSentenceLength):\n",
    "            sent = sent + \" \" + padToken\n",
    "            sentLength +=1\n",
    "        paddedSents.append(sent)\n",
    "    assert(min(sentLengths) >0)\n",
    "    assert(max(sentLengths)<=maxSentenceLength)\n",
    "    return(paddedSents,sentLengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extend length of an indicator vector so that all indicator vectors are the same as the longest vector ###\n",
    "This is needed so that all indicator vectors can be combined into a matrix, letting tensorflow vectorize deep learning operations <br>\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inputVectors** (list of integer arrays) - indicator vectors that need to be padded\n",
    "- **maxSentenceLength** (int) - number of elements each vector must contain.  Add zeros to the end of shorter sentences to increase their length <br>\n",
    "\n",
    "**Outputs:** <br>\n",
    "- **paddedVectors** - revised version of inputVectors, with padding added so all vectors have maxSentence length number of elements <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padIndicatorVector(inputVectors,maxSentenceLength):\n",
    "    paddedVectors = []\n",
    "    for vector in inputVectors:\n",
    "        vectorLength = len(vector)\n",
    "        numVector = np.asarray(list(map(int, vector)),dtype=np.int32).reshape((vectorLength))\n",
    "        if(maxSentenceLength-vectorLength >0):\n",
    "            padding = np.zeros((maxSentenceLength-vectorLength,1),dtype=np.int32)\n",
    "            numVector = np.append(numVector,padding)\n",
    "            numVector = numVector.reshape(maxSentenceLength)\n",
    "        paddedVectors.append(numVector)\n",
    "    return(paddedVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add padding to all input features to the tensorflow model ###\n",
    "This ensures all vectors are the same length, greatly increasing tensorflow's ability to vectorize operations and reduce training time.\n",
    "\n",
    "**Inputs:** <br>\n",
    "- **inputSentences** (list of string arrays) - sentences that need to be padded <br>\n",
    "- **inputHashtagInd** (list of integer arrays) - hashtag vectors that need to be padded <br>\n",
    "- **inputEmoticonInd** (list of integer arrays) - emoticon vectors that need to be padded <br>\n",
    "- **padToken** (string) - token word to add at end of sentences to make their length the same <br>\n",
    "\n",
    "**Outputs:** <br>\n",
    "- **paddedSents** (list of string arrays) - revised version of inputSentences, with padding added so all sentences have maxSentenceLength number of words <br>\n",
    "- **sentLengths** (int array) - length of each sentence in inputSentences **before** adding padding <br>\n",
    "- **paddedHashtagInds** - revised version of inputHashtagInd, with padding added so all vectors have maxSentence length number of elements <br>\n",
    "- **paddedEmoticonInds** - revised version of inputEmoticonIn, with padding added so all vectors have maxSentenceLength number of elements <br>\n",
    "- **maxSentenceLength** - number of elements each vector contains after padding <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPadding(inputSentences,inputHashtagInd,inputEmoticonInd,padToken):\n",
    "    paddedDict = {'maxSentLength': findMaxSentenceLength(inputEmoticonInd) }\n",
    "    paddedDict['paddedSents'], paddedDict['sentLengths'] = padSentences(inputSentences,maxSentenceLength,padToken)\n",
    "    paddedDict['paddedHashtags'] = padIndicatorVector(inputHashtagInd,maxSentenceLength)\n",
    "    paddedDict['paddedEmots'] = padIndicatorVector(inputEmoticonInd,maxSentenceLength)\n",
    "    return(paddedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save output files in pickled format\n",
    "def pickleOutputFiles(pickleParams,devDict,testDict,trainDict,allDict,embeddingMatrix,word2Index):\n",
    "    pickle.dump(devDict,open(pickleParams['devDictPicklePath'],\"wb\" ))\n",
    "    pickle.dump(testDict,open(pickleParams['testDictPicklePath'],\"wb\" ))\n",
    "    pickle.dump(trainDict,open(pickleParams['trainDictPicklePath'],\"wb\" ))\n",
    "    pickle.dump(allDict,open(pickleParams['allDictPicklePath'],\"wb\"))\n",
    "    pickle.dump(embeddingMatrix,open(pickleParams['embeddingMatrixPicklePath'],\"wb\" ))\n",
    "    pickle.dump(word2Index,open(pickleParams['word2IndexPicklePath'],\"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingWeights = loadWordVec(wordVecParams)\n",
    "tweetDict = loadWordList(twitterCSVParams)\n",
    "# print(tweetDict.head()) # for debug/ validation purposes only\n",
    "screenedSents,screenedHashtags,screenedEmots = screenForCompactedWordsAllTweets(tweetDict['clean_text'],embeddingWeights,tweetDict['hash_ind'],tweetDict['emot_ind'],True)\n",
    "index2WordMap, word2IndexMap = generateWordMap(screenedSents)\n",
    "word2embeddingDict = reduceWordEmbeddings(embeddingWeights,index2WordMap)\n",
    "#countUnknownWords(word2embeddingDict,screenedSents) # for debug/ validation purposes only\n",
    "word2embeddingDict['UNK'] = np.random.randn(modelParams['word_vec_dim'])*0.001 # initialize to small but non-zero random weight vector\n",
    "word2embeddingDict['PAD_TOKEN'] = np.random.randn(modelParams['word_vec_dim'])*0.001 # initialize to small but non-zero random weight vector\n",
    "embeddingMatrix,word2Index = convertVectorsToMatrix(word2embeddingDict,modelParams['word_vec_dim'])\n",
    "taggedSentences = substituteAllUnknownTags(screenedSents,\"UNK\",word2Index,unknownWordSubs)\n",
    "taggedSentences = substituteAllUnknownTags(screenedSents,\"UNK\",word2Index)\n",
    "paddedDict = addPadding(taggedSentences, screenedHashtags, screenedEmots, \"PAD_TOKEN\")\n",
    "paddedSent, sentLengths,paddedHashtags,paddedEmoticons,maxSentenceLength = addPadding(taggedSentences, screenedHashtags, screenedEmots, \"PAD_TOKEN\")\n",
    "paddedDict['twoClassLabels'] = expandAllClassLabels(tweetDict, twitterCSVParams)\n",
    "twoClassLabels = expandAllClassLabels(tweetDict, twitterCSVParams)\n",
    "screenedDict = applyExclusionCriteria(paddedDict)\n",
    "devDict,testDict,trainDict,allDict = splitTrainDevTest(modelParams,screenedDict)\n",
    "# commented out to prevent overriding original datasets during code optimization\n",
    "#pickleOutputFiles(picleParams,devDict,testDict,trainDict,allDict,embeddingMatrix,word2Index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to calculate descriptive statistics of train, dev, and test sets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# descriptive statistics for sentence length\n",
    "def calcSentLengthStats(inDict,indicatorKey):\n",
    "    sentDict = {'avgSentLength':np.mean(np.array(inDict[indicatorKey])) }\n",
    "    sentDict['stdDevSentLength'] = np.std(np.array(inDict[indicatorKey]))\n",
    "    sentDict['maxSentLength'] = max(inDict[indicatorKey])\n",
    "    sentDict['minSentLength'] = min(inDict[indicatorKey])\n",
    "    sentDict['percentiles'] = np.percentile(np.array(inDict[indicatorKey]),[5,25,50,75,95])\n",
    "    return sentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# descriptive statistics for word indicators (e.g. if a word is in a hashtag)\n",
    "def calcStatsWordIndicator(inDict,indicatorKey):\n",
    "    tempDict = {'numPositive_' + indicatorKey + 'Records':sum(np.max(np.array(inDict[indicatorKey]),axis=1)) }\n",
    "    tempDict['max_' + indicatorKey + 'Words'] = max(np.sum(np.array(inDict[indicatorKey]),axis=1))\n",
    "    tempDict['min_' + indicatorKey + 'Words'] = min(np.sum(np.array(inDict[indicatorKey]),axis=1))\n",
    "    subsetIndices = np.where(np.sum(np.array(inDict[indicatorKey]),axis=1)>0)\n",
    "    subsetRecords = np.take(inDict[indicatorKey],subsetIndices,axis=0).reshape(\n",
    "        subsetIndices[0].size,len(inDict[indicatorKey][0]))\n",
    "    tempDict['avg_' + indicatorKey + 'Words'] = np.mean(np.sum(subsetRecords,axis=1))#np.sum(np.array(inDict[indicatorKey]),axis=1))\n",
    "    tempDict['stdDev_' + indicatorKey + 'Words'] = np.std(np.sum(subsetRecords,axis=1))\n",
    "    tempDict[indicatorKey + '_percentiles'] = np.percentile(np.sum(subsetRecords,axis=1),[5,25,50,75,95])\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# descriptive statistics for outcome class labels\n",
    "def calcOutcomeStats(inDict,outcomes):\n",
    "    outcomeStatDict = {'outcomes':outcomes}\n",
    "    numRecords = len(inDict['x'])\n",
    "    outcomeIndices = [0,2,4,6,8,10,12]\n",
    "    outcomeStatDict['frequencies'] = inDict['y'][:,outcomeIndices].sum(axis=0)\n",
    "    outcomeStatDict['percent'] = (outcomeStatDict['frequencies']*100.0) / numRecords\n",
    "    numVals = inDict['y'][:,[0,2,4,6,8,10,12]].sum(axis=1)\n",
    "    numMultipleLabels = {}\n",
    "    for i in range(8):\n",
    "        outcomeStatDict[str(i) + \"labels\"] = sum(numVals==i)\n",
    "    return(outcomeStatDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsetDictionaryByNatureVal(inDict,valToKeep):\n",
    "    natureLabels = inDict['y'][:,[0]]\n",
    "    totalRecordsToKeep = sum(natureLabels == valToKeep)\n",
    "    natureIndices = np.where(natureLabels==valToKeep)[0]\n",
    "    keys = inDict.keys()\n",
    "    subsetDict = {}\n",
    "    for key in keys:\n",
    "        keeps = np.array([inDict[key][i] for i in natureIndices])\n",
    "        subsetDict[key] = keeps\n",
    "        assert(len(subsetDict[key]) == totalRecordsToKeep)\n",
    "    return subsetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all descriptive statistics for one dataset\n",
    "def calcDescriptiveStatsOneDataset(inDict):\n",
    "    outcomes = ['nature','safety','beauty','exercise','social','stress','air']\n",
    "    outcomeStat = calcOutcomeStats(inDict,outcomes)\n",
    "    sentStats = calcSentLengthStats(inDict,'seqlens')\n",
    "    hashStats = calcStatsWordIndicator(inDict,'hash')\n",
    "    emotStats = calcStatsWordIndicator(inDict,'emot')\n",
    "    statDict = {'outcomeStats':outcomeStat,'sentStats':sentStats,'hashStats':hashStats,'emotStats':emotStats}\n",
    "    return statDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc descriptive stats for all datasets\n",
    "allDictStats = calcDescriptiveStatsOneDataset(allDict)\n",
    "allDictNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(allDict,1))\n",
    "allDictNotNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(allDict,0))\n",
    "trainDictStats = calcDescriptiveStatsOneDataset(trainDict)\n",
    "trainDictNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(trainDict,1))\n",
    "trainDictNotNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(trainDict,0))\n",
    "devDictStats = calcDescriptiveStatsOneDataset(devDict)\n",
    "devDictNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(devDict,1))\n",
    "devDictNotNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(devDict,0))\n",
    "testDictStats = calcDescriptiveStatsOneDataset(testDict)\n",
    "testDictNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(testDict,1))\n",
    "testDictNotNatureStats = calcDescriptiveStatsOneDataset(subsetDictionaryByNatureVal(testDict,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
