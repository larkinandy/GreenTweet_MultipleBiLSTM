{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate and graph performance summary statistics #\n",
    "\n",
    "** Author: Andrew Larkin **, Oregon State University College of Public Health and Human Sciences <br>\n",
    "** Date created: ** January 5th, 2018\n",
    "\n",
    "### Summary ###\n",
    "For evaluating performance of candidate models in train, dev, test, and independent datasets.  Calculate confusion matrices of model-dataset combinations.  Graph precision and recall for each model-dataset combation and outcome.\n",
    "\n",
    "This script is divided into two parts: <br>\n",
    "1) Calculate performance metrics and output to CSV <br>\n",
    "2) Graph performance metrics and save to .eps file (or print to screen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define global variables and constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math, os, pickle, re, string\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input and output filepaths\n",
    "parentFolder = \"C:/Users/larkinan/Desktop/DBTraining/\"\n",
    "performFolder = parentFolder + \"modelTrainingPerformance/\"\n",
    "datasetFolder = performFolder + \"preprocessingOutput/\"\n",
    "performCSV_Denorm = performFolder + \"ModelPrecisionRecall_Jan6_18.csv\"   # intermediate file containing precision and recall estimates to graph\n",
    "\n",
    "# pickled datasets to load\n",
    "datasetPickleParams = { # where to store datasets for model training on hard disk\n",
    "                        \"trainDictPicklePath\":dataset + \"trainDict.p\",\n",
    "    \"devDictPicklePath\":dataset + \"devDict.p\",\n",
    "    \"testDictPicklePath\":dataset + \"testDict.p\",\n",
    "    \"allDictPicklePath\":dataset + \"allDict.p\",\n",
    "    \"embeddingMatrixPicklePath\":dataset + \"embeddingMatrix.p\",\n",
    "    \"word2IndexPicklePath\":dataset + \"word2Index.p\"\n",
    "    \"NYC_DictPicklePath\":dataset + \"NYC_Dict.p\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Calculate performance metrics and output to CSV ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pickled preprocessed data\n",
    "def loadDatasets(pickleParams):\n",
    "    trainDict = pickle.load(open(pickleParams['trainDictPicklePath'],'rb'))\n",
    "    devDict = pickle.load(open(pickleParams['devDictPicklePath'],'rb'))\n",
    "    testDict = pickle.load(open(pickleParams['testDictPicklePath'],'rb'))\n",
    "    NYC_Dict = pickle.load(open(pickleParams['NYC_DictPicklePath'],'rb'))\n",
    "    embeddingMatrix = pickle.load(open(pickleParams['embeddingMatrixPicklePath'],'rb'))\n",
    "    word2IndexMap = pickle.load(open(pickleParams['word2IndexPicklePath'],'rb'))\n",
    "    return(trainDict,devDict,testDict,NYC_Dict,embeddingMatrix,word2IndexMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  randomly sample record indices \n",
    "def getSampledIndices(dataX):\n",
    "    instanceIndices = list(range(len(dataX)))\n",
    "    np.random.shuffle(instanceIndices)\n",
    "    sampledIndices = instanceIndices[:batchSize]\n",
    "    return(sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sampled text and convert to index values using the map\n",
    "def getSampledXVals(sampledIndices,dataX,word2IndexMap):\n",
    "    sampledX = []\n",
    "    for i in sampledIndices:\n",
    "        sent = dataX[i]\n",
    "        tempX = []\n",
    "        for word in sent.split():\n",
    "            \n",
    "            # when applying to datasets other than the train, dev, and test, some words may not be in the dictionary\n",
    "            if(word in word2IndexMap):\n",
    "                tempX.append(word2IndexMap[word])\n",
    "            else:\n",
    "                tempX.append(word2IndexMap['UNK'])\n",
    "        sampledX.append(tempX)\n",
    "    return(sampledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get a random sample for a single epoch or evaluation###\n",
    "**Inputs**: <br>\n",
    "- **batchSize** (int) - number of records to randomly sample <br>\n",
    "- **dataX** (string array) - tweet text for all records <br>\n",
    "- **dataY** (array of 1x7 int arrrays) - each 1x7 int array corresponds to 7 labels for one record <br>\n",
    "- **dataSeqLens** (int array) - number of words in each record\n",
    "- **dataHash** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a hashtag <br>\n",
    "- **dataEmot** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from an emoticon <br>\n",
    "- **dataLoc** (array of binary numbers) - the nth digit in the ith binary number indicates whether the nth word in the ith dataX record is from a regional location description <br>\n",
    "- **word2IndexMap** (dict) - dictionary of word:index keys <br>\n",
    "- **numOutcomes** (int) - number of outcomes in the dataset <br>\n",
    "\n",
    "**Outputs**: <br>\n",
    "- **sampledX** (array of int arrays) - word2Index mapped numbers for the words in the sampled tweets <br>\n",
    "- **sampledY** (array of int arrays) - outcome labels for sampled tweets <br>\n",
    "- **samplesdSeqLens** (int array) - length of of sampled tweets <br>\n",
    "- **sampledHash** (array of int arrays) - indicator values of which words in the sampled tweets are hashtags <br>\n",
    "- **sampledEmot** (array of int arrays) - indicator values of which words in the sampled tweets are emoticon descriptions <br>\n",
    "- **sampledLoc** (array of int arrays) - indicator values of which words in the sampled tweets are regional descriptions that use nature-related vocabulary <br>\n",
    "- **sampledIndices** (int array) - sampled record indices in the original dataset <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceBatch(batchSize,dataX,dataY,\n",
    "                       dataSeqlens,dataHash,dataEmot,dataLoc,\n",
    "                       word2IndexMap,numOutcomes):\n",
    "    \n",
    "    sampledIndices = getSampledIndices(dataX)\n",
    "    sampledX = getSampledXVals(sampledIndices,dataX,word2IndexMap)\n",
    "    sampledY = np.asarray([dataY[i][0:numOutcomes*2] for i in sampledIndices]).reshape((batchSize, numOutcomes*2))\n",
    "    sampledSeqlens = [dataSeqlens[i] for i in batch]\n",
    "    sampledHash = np.asarray([dataHash[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(hash_data[0]),1))\n",
    "    sampledEmot = np.asarray([emot_data[i] for i in sampledIndices],dtype=np.float32).reshape((batchSize,len(emot_data[0]),1))\n",
    "    sampledLoc = np.asarray([loc_data[i] for i in sampledIndices],dtype=np.float32).reshape((batbatchSize,len(loc_data[0]),1))\n",
    "    \n",
    "    return(sampledX,sampledY,sampledSeqlens,sampledHash,sampledEmot,sampledLoc,sampledIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract vectors from dataset dictionary.  \n",
    "def extractDataFromDict(inputDict):\n",
    "    return(inputDict['sent'], inputDict['labels'], inputDict['seqLens'],\n",
    "           inputDict['hash'], inputDict['emot'],inputDict['loc_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update performance mat once values are calculated\n",
    "def fillInPerfMat(performanceMat,MCC,F1Score,Precision,Recall,TP,FP,TN,FN):\n",
    "    performanceMat[perfMatIndex,0] = MCC\n",
    "    performanceMat[perfMatIndex,1] = F1Score\n",
    "    performanceMat[perfMatIndex,2] = Precision\n",
    "    performanceMat[perfMatIndex,3] = Recall\n",
    "    performanceMat[perfMatIndex,4] = TP\n",
    "    performanceMat[perfMatIndex,5] = TN\n",
    "    performanceMat[perfMatIndex,6] = FP\n",
    "    performanceMat[perfMatIndex,7] = FN\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMCC(TPperc,TNperc,FPperc,FNperc):\n",
    "    MCCNumerator = TPperc*TNperc-FPperc*FNperc\n",
    "\n",
    "\n",
    "    MCCDenomenator = math.sqrt((TPperc+FPperc)*(TPperc+FNperc)*(TNperc+FPperc)*(TNperc+FNperc))\n",
    "    MCC = 0\n",
    "    try:\n",
    "        MCC = MCCNumerator/MCCDenomenator\n",
    "    except:\n",
    "        MCC = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate statistics for one outcome of a subset of the entire epoch dataset ###\n",
    "**Inputs**: <br>\n",
    "- **subsetLabels** (int array) - labels for the outcome of interest and subset of interest.  value of 1 corresponds to a prediction of a positive label (label array holds actual labels) <br>\n",
    "- **predictionSubset** (int array) - model predictions for the outcome of interest and subset of interest. Prediction array corresponds to the predicted column that the model predicts - column 0 holds the positive label, column 1 holds the negative label.  A value of 0 in predictionSubset therefore corresponds to predicting a positive label <br>\n",
    "- **performanceMat** (float matrix) - a nx8 matrix, where n corresponds to the number of outcomes and 8 corresponds to the number of performance metrics the matrix holds <br>\n",
    "- **perMatIndex** (int) - index of the current outcome (and row of the performanceMat) to calculate performance values for <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcStatsForSubset(subsetLabels,predictionSubset,performanceMat,perfMatIndex):\n",
    "    \n",
    "    TP, TN, FP, FN, TPperc,FPperc,TNperc,FNperc, Recall, Precision, F1Score = [0 for x in range(11)]\n",
    "    \n",
    "    \n",
    "    positivePredictionLabels = subsetLabels[np.where( predictionSubset == 0 )]   \n",
    "    \n",
    "    if(len(positivePredictionLabels)>0):\n",
    "        TP = np.sum(np.equal(positivePredictionLabels,1))\n",
    "        FP = len(positivePredictionLabels) - TP\n",
    "        \n",
    "    # calc TN and FN\n",
    "    negativePredictionLabels = subsetLabels[np.where( predictionSubset == 1 )] \n",
    "    \n",
    "    if(len(negativePredictionLabels) > 0):\n",
    "        TN = np.sum(np.equal(negativePredictionLabels,0))\n",
    "        FN = len(negativePredictionLabels) - TN\n",
    "    if(TP > 0):\n",
    "        Recall = (TP*1.0)/((TP+FN)*1.0)\n",
    "        Precision = (TP*1.0)/((TP+FP)*1.0)\n",
    "        F1Score = 2*(Precision*Recall)/(Precision+Recall)\n",
    "        TPperc = float(TP/len(predictionSubset))*100\n",
    "    if(FP>0):\n",
    "        FPperc = float(FP/len(predictionSubset))*100\n",
    "    if(TN>0):\n",
    "        TNperc = float(TN/len(predictionSubset))*100\n",
    "    if(FN>0):\n",
    "        FNperc = float(FN/len(predictionSubset))*100\n",
    "\n",
    "    MCC = calcMcc(TPperc,TNperc,FPperc,FNperc)\n",
    "    \n",
    "    fillInPerfMat(performanceMat,MCC,F1Score,Precision,Recall,TP,FP,TN,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert float matrix to dictionary\n",
    "def perfMatToDict(inMat):\n",
    "    performanceDict = { 'MCC':inMat[:,0],\n",
    "                        'F1Score':inMat[:,1],\n",
    "                        'Precision':inMat[:,2],\n",
    "                        'Recall':inMat[:,3],\n",
    "                        'TP':inMat[:,4],\n",
    "                        'TN':inMat[:,5],\n",
    "                        'FP':inMat[:,6],\n",
    "                        'FN':inMat[:,7]\n",
    "                        }\n",
    "    return(performanceDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate confusion matrix for an epoch or evaluation run, including subsets for greenspace and non greenspace outcomes ###\n",
    "**Inputs**: <br>\n",
    "- **prediction** (int array) - array of prediction values.  Values are flattened into a 1 x (n*m) array, where n is the number of records and m is the number of outcomes <br>\n",
    "- **labels** (array of int arrays) - labels of each outcome.  Each int array contains nx2 ints, where n is the number of outcomes.  See the data preprocessing section for more details about the label structure <br>\n",
    "- **batchSize** (int) - number of records <br>\n",
    "- **numOutcomes** (int) - number of outcomes <br>\n",
    "\n",
    "**Outputs**: <br>\n",
    "- **allDict** (dict) - performance dictionary for all records and all outcomes <br>\n",
    "- **greenDict** (dict) - performance dictionary for records with positive greenspace labels <br>\n",
    "- **notGreenDict** (dict) - performance dictionary for records with negative greenspace labels <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcConfMatrix(prediction,labels,batchSize,numOutcomes):\n",
    "\n",
    "    # Create confusion matrices for all data, data with positive green labels, and data with \n",
    "    # negatie green labels.  8 performance metrics for each outcome\n",
    "    performanceMatGreen = np.zeros((numOutcomes,8))  \n",
    "    performanceMatNotGreen = np.zeros((numOutcomes,8))\n",
    "    performanceMatAll = np.zeros((numOutcomes,8))\n",
    "    positiveGreenIndeces = []\n",
    "    negativeGreenIndeces = []\n",
    "    labelOffset = 7 - numOutcomes\n",
    "    \n",
    "    \n",
    "    # calculate statistics for one outcome at a time\n",
    "    for predictionIndex in range(numOutcomes):\n",
    "        subsetLabels = labels[:,(predictionIndex+labelOffset)*2]\n",
    "\n",
    "        predictionSubset = prediction[batchSize*(predictionIndex):batchSize*(predictionIndex+1)]\n",
    "        \n",
    "        # prediction index 0 corresponds to the first outcome, which is greenspace.  Use this to identify \n",
    "        # which records have positive greenspace labels\n",
    "        if(predictionIndex==0):\n",
    "            positiveGreenIndeces = np.where(predictionSubset==0)\n",
    "            negativeGreenIndeces = np.where(predictionSubset==1)\n",
    "        else:\n",
    "            positiveGreenSubset = subsetLabels[positiveGreenIndeces]\n",
    "            negativeGreenSubset = subsetLabels[negativeGreenIndeces]\n",
    "\n",
    "            positiveGreenPredictions = predictionSubset[positiveGreenIndeces]\n",
    "            positivePredictionLabels = positiveGreenSubset[np.where( positiveGreenPredictions == 0 )]\n",
    "            negativeGreenPredictions = predictionSubset[negativeGreenIndeces]\n",
    "            \n",
    "            calcStatsForSubset(positiveGreenSubset,positiveGreenPredictions,performanceMatGreen,predictionIndex)\n",
    "            calcStatsForSubset(negativeGreenSubset,negativeGreenPredictions,performanceMatNotGreen,predictionIndex)\n",
    "        calcStatsForSubset(subsetLabels,predictionSubset,performanceMatAll,predictionIndex)\n",
    "\n",
    "    # convert performance matrices to dictionaries\n",
    "    allDict = perfMatToDict(performanceMatAll)\n",
    "    greenDict = perfMatToDict(performanceMatGreen)\n",
    "    notGreenDict = perfMatToDict(performanceMatNotGreen)\n",
    "    \n",
    "    return([allDict,greenDict,notGreenDict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify records where predictions don't match labels (false positives or fase negatives) for all outcomes ###\n",
    "**Inputs**: <br>\n",
    "- **text** (string array) - text of each record <br>\n",
    "- **yTest** (array of int arrays) - labels for each outcome.  NOTE: 1 corresponds to a positive label, while one corresponds to a negative label.  This differs from yPredict. One int array corresponds to the outcome labels for record. <br>\n",
    "- **yPredict** (array of int arrays) - predictions for each outcome.  NOTE: 1 corresponds to a negative label, while 0 corresponds to a positive label.  This differs from yTest.  One int array corrresponds to the outcome labels for once record <br>\n",
    "- **batchSize** (int) - number of records <br>\n",
    "- **idNums** (int array) - unique id number of each record from the pickled database loaded from the hard drive <br>\n",
    "- **falseNum** (int) - number that the yPredict and yTest will both hold if the record is false.  For example, if falseNum is 1 then the function is identifying flase negatives, and yTest and yPredict will both contain 1 <br>\n",
    "\n",
    "**Outputs**: <br>\n",
    "- **masterDictArray** (array of dicts) - one dictionary for each outcome.  Each dictionary contains the following key:value pairs:\n",
    "\n",
    "    1) FalseID (int array) - unique id numbers for each mismatch record.  Id numbers correspond to original pickled dataset loaded from secondary storage <br>\n",
    "    2) FalseText (string array) - text for each mistmatch record <br>\n",
    "    3) Outcome (int) - index of the outcome.  0 for greenspace, 7 for air. See the data preprocessing section for more details <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identifyFalseRecords(text,yTest,yPredict,batchSize,idNums,falseNum):\n",
    "  \n",
    "    numOutcomes = int(len(yTest[0])/2)\n",
    "    masterDictArray = []\n",
    "  \n",
    "    for i in range(numOutcomes):\n",
    "        subsetLabels = yTest[:,i*2]\n",
    "        predSubset = yPredict[batchSize*(i):batchSize*(i+1)]\n",
    "        \n",
    "        # identify id numbers of records where labels and predictions both have the false num value \n",
    "        #(i.e. don't predict the same label for the current outcome)\n",
    "        posPredIndices = np.where(predSubset == falseNum)\n",
    "        negActual = np.where(subsetLabels == falseNum)\n",
    "        \n",
    "        FP_ID = []\n",
    "        FP_X = []\n",
    "        flatPred = posPredIndices[0].ravel().tolist()\n",
    "        \n",
    "        # iteratively search through predictions.  If id number is in both, then there's a mismatch between\n",
    "        # prediction and label\n",
    "        for index in range(len(flatPred)):\n",
    "            candVal = flatPred[index]\n",
    "            if candVal in negActual[0]:\n",
    "                origId = id_nums[candVal]\n",
    "                FP_ID.append(origId)\n",
    "                FP_X.append(X[origId])\n",
    "        \n",
    "        tempDict = {'FalseID':FP_ID,'FalseText':FP_X,'Outcome':i}\n",
    "        masterDictArray.append(tempDict)\n",
    "        \n",
    "    return(masterDictArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dictionary to pandas dataframe and save to csv file.  outFilepath is an absolute filepath\n",
    "def saveDictToCSV(inDict,outFilepath):\n",
    "    performanceDF = ps.DataFrame.from_dict(inDict)\n",
    "    performanceDF.to_csv(outFilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### identify false predictions and save text to csv file ###\n",
    "**Inputs**:\n",
    "- **text** (string array) - text of each record <br>\n",
    "- **yTest** (array of int arrays) - labels for each outcome.  NOTE: 1 corresponds to a positive label, while one corresponds to a negative label.  This differs from yPredict. One int array corresponds to the outcome labels for record. <br>\n",
    "- **predVals** (array of int arrays) - predictions for each outcome.  NOTE: 1 corresponds to a negative label, while 0 corresponds to a positive label.  This differs from yTest.  One int array corrresponds to the outcome labels for once record <br>\n",
    "- **batchSize** (int) - number of records <br>\n",
    "- **indexNums** (int array) - unique id number of each record from the pickled database loaded from the hard drive <br>\n",
    "- **iterationNum** (int) - model run number, when multiple batches are needed for large datasets too big to fit into a single model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFalsePredictionText(text,yTest,predVals,batchSize,indexNums,iterationNum):\n",
    "    # get the text for the false positive and false negatives\n",
    "    dictLabels = ['FN','FP']\n",
    "    FalseNegDict = identifyFalseRecords(text,yTest,predVals,batchSize,indexNums,1)\n",
    "    FalsePosDict = identifyFalseRecords(text,yTest,predVals,batchSize,indexNums,0)\n",
    "    dicts = [FalseNegDict,FalsePosDict]\n",
    "    \n",
    "    outcomes = ['greenspace','safety','beauty','exercise','social','stress','air']\n",
    "    # for each outcome and dictionary of false positives and negatives, write text and metadata to csv\n",
    "    for dictIndex in range(len(dicts)):\n",
    "        label = dictLabels[dictIndex]\n",
    "        tempDict = dicts[dictIndex]\n",
    "        for outcomeIndex in range(len(outcomes)):\n",
    "            outputFilename = performFolder + \"%s%s%i.csv\" %(outcomes[outcomeIndex],label,iterationNum)\n",
    "            if(not os.path.exists(outputFilename)):\n",
    "                saveDictToCSV(tempDict[outcomeIndex],outputFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate model performance statistics, and get text of misclassified tweets ###\n",
    "**Inputs**: <br>\n",
    "- **text** (string array) - text of each record <br>\n",
    "- **yTest** (array of int arrays) - labels for each outcome.  NOTE: 1 corresponds to a positive label, while one corresponds to a negative label.  This differs from yPredict. One int array corresponds to the outcome labels for record. <br>\n",
    "- **predVals** (array of int arrays) - predictions for each outcome.  NOTE: 1 corresponds to a negative label, while 0 corresponds to a positive label.  This differs from yTest.  One int array corrresponds to the outcome labels for once record <br>\n",
    "- **batchSize** (int) - number of records <br>\n",
    "- **indexNums** (int array) - unique id number of each record from the pickled database loaded from the hard drive <br>\n",
    "- **iterationNum** (int) - model run number, when multiple batches are needed for large datasets too big to fit into a single model <br>\n",
    "- **numOutcomes** (int) - number of outcome classes <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the confusion matrix, MCC, F1Score, Precision, and Recall for all outcomes  \n",
    "def getModelPerfStats(text,predVals,yTest,batchSize,indexNums,iterationNum,outType,numOutcomes):\n",
    "    \n",
    "    # calculate confucsion matrix for the entire dataset, as well as subset with positive and negative \n",
    "    # greenspace labels\n",
    "    [perfDictAll,perfDictGreen,perfDictNotGreen] = (calcConfMatrix(predVals,yTest,batchSize,numOutcomes))\n",
    "    saveDictToCSV(perfDictAll,performFolder + outType + \"performAll.csv\")\n",
    "    saveDictToCSV(perfDictGreen,performFolder + outType + \"performGreen.csv\")\n",
    "    saveDictToCSV(perfDictNotGreen,performFolder + outType + \"performNotGreen.csv\")\t\n",
    "    \n",
    "    getFalsePredictionText(text,yTest,predVals,batchSize,indexNums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model properties from input tensorflow metadata objecxt\n",
    "def loadModelProperties(inputModelMeta):\n",
    "    \n",
    "    model_io = tf.get_collection('model_io')\n",
    "    inputs = inputModelMeta[0]\n",
    "    labels = inputModelMeta[1]\n",
    "    seqlens = inputModelMeta[2]\n",
    "    hash_ind = inputModelMeta[3]\n",
    "    emot_ind = inputModelMeta[4]\n",
    "    loc_ind = inputModelMeta[5]\n",
    "    prediction = inputModelMeta[6]\n",
    "    cost = model_io[7]\n",
    "    \n",
    "    return(inputs,labels,seqlens,hash_ind,loc_ind,prediction,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load trained model into memory and classify predictions ###\n",
    "**Inputs**: <br>\n",
    "- **inDict** (dict) - dictionary containing input data for model, and labels to test model performance <br>\n",
    "- **batchSize** (int) - number of records in inDict <br>\n",
    "- **outType** (string) - tag to add to saved records, indicating what type of data (e.g. train, dev) inDict corresponds to <br>\n",
    "- **modelNum** (int) - unique id signifying which trained model to load into memory.  This is the original epoch number during the traing process <br>\n",
    "- **numOutcomes** (int) - number of outcomes in the model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(inDict,batchSize,outType,modelNum,numOutcomes):\n",
    "    X,Y,Seqlens,Hash,Emot,Loc = extractDataFromDict(inDict)\n",
    "    x_test,y_test,seqlen_test,hashtag_test,emot_test,loc_test,indexNums = get_sentence_batch(batchSize,X,Y,Seqlens,Hash,Emot,Loc,word2IndexMap,7,0)\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(performFolder + \"model_io.meta\")\n",
    "        saver.restore(sess,performFolder + \"model-\" + str(modelNum))\n",
    "\n",
    "        inputs, labels, seqlens, hash_ind,loc_ind,prediction,cost = loadModelProperties(model_io)\n",
    "        \n",
    "        # script is running on NDIVIA Titan V (12GB memory) and Titan X Pascal video cards.  \n",
    "        # Upper limit for number of records in a single run is approximately 10,000.\n",
    "        numModelRuns = math.ceil(batchSize/10000)\n",
    "\n",
    "        for modelRun in range(numModelRuns):\n",
    "\n",
    "            x_test_partition = x_test[modelRun*10000:(modelRun+1)*10000]\n",
    "            y_test_partition = y_test[modelRun*10000:(modelRun+1)*10000]\n",
    "            seqlen_test_partition =seqlen_test[modelRun*10000:(modelRun+1)*10000]\n",
    "            hashtag_test_partition = hashtag_test[modelRun*10000:(modelRun+1)*10000]\n",
    "            emot_test_partition = emot_test[modelRun*10000:(modelRun+1)*10000]\n",
    "            loc_test_partition = loc_test[modelRun*10000:(modelRun+1)*10000]\n",
    "            indexNums_partition = indexNums[modelRun*10000:(modelRun+1)*10000]\n",
    "\n",
    "            pred_vals,c = sess.run([prediction,cost],\n",
    "                                   feed_dict={inputs:x_test_partition,\n",
    "                                              labels:y_test_partition,\n",
    "                                              seqlens:seqlen_test_partition,\n",
    "                                              hash_ind:hashtag_test_partition,\n",
    "                                              emot_ind:emot_test_partition,\n",
    "                                              loc_ind:loc_test_partition\n",
    "                                              })\n",
    "\n",
    "            # calculate model performance and write to stable storage\n",
    "            getModelPerfStats(X,pred_vals,y_test_partition,len(indexNums_partition),indexNums_partition,modelRun,outType,numOutcomes,saveToCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main script in part  1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDict, devDict, testDict, NYC_Dict,embeddingMatrix, word2IndexMap = loadDatasets(datasetv2PickleParams)\n",
    "runModel(trainDict,60000,\"train\",21500,7)\n",
    "runModel(devDict,5000,\"dev\",21500,7)\n",
    "runModel(testDict,5000,\"test\",21500,7)\n",
    "runModel(NYCDict,4850,\"NYC\",21500,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Graph Performance Metrics ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load performanceMeasuresFromCSV\n",
    "def loadPerformanceData(dataFilepath,debug=False):\n",
    "    rawData = ps.read_csv(dataFilepath)\n",
    "    \n",
    "    # Second step in case future versions need to process input data\n",
    "    processedData = rawData\n",
    "    \n",
    "    if(debug):\n",
    "        print(processedData.head())\n",
    "        keys = processedData.keys()\n",
    "        print(\"number of records: %i\" %len(processedData[keys[0]]))\n",
    "    \n",
    "    return(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each subplot, setup graph properties including axis boundaries, titles, and reference lines ###\n",
    "**Inputs:** <br>\n",
    "- **tempAxis** (object) - matplotlib axis object for the current subplot <br>\n",
    "- **xDim** (2 element float array) - min and max boundaries for the x axis\n",
    "- **yDim** (2 element float array) - min and max boundaries for the y axis\n",
    "- **yLabel** (string) - optional label for the yaxis\n",
    "- **xLabel** (string) - optional label for the xaxis\n",
    "- **outcome** (string) - optional subplot title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setAxisProperties(tempAxis,xDim,yDim,yLabel=None,xLabel = None,title=None):\n",
    "    v_line = mlines.Line2D([0.7, 0.7], [0, 1], color='black',linestyle='dashed')\n",
    "    h_line = mlines.Line2D([0,1],[0.7,0.7],color='black',linestyle='dashed')\n",
    "    tempAxis.set_xlim(xDim)\n",
    "    tempAxis.set_ylim(yDim)\n",
    "    tempAxis.add_line(v_line)\n",
    "    tempAxis.add_line(h_line)\n",
    "    if(yLabel):\n",
    "        tempAxis.set_ylabel(yLabel)\n",
    "    if(xLabel): \n",
    "        tempAxis.set_xlabel(xLabel)\n",
    "    if(title):\n",
    "        tempAxis.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot subset for one outcome of interest  ### \n",
    "plot includes all models and datasets for the outcome.  Colors and markers correspond to dataset and model, respectively <br>\n",
    "\n",
    "**Inputs** <br>\n",
    "- **performData** (pandas dataframe) - contains denormalized records of the dataset.  Important keys in the dataset include: <br>\n",
    "    1) outcome - which outcome the record corresponds to <br>\n",
    "    2) dataset - which dataset the record corresponds to (e.g. train, dev, test, etc.) <br>\n",
    "    3) model - which model the record corresponds to <br>\n",
    "    4) precision <br>\n",
    "    5) recall <br>\n",
    "    \n",
    "    \n",
    "- **outcome** (string) - outcome of interest.  Used to screen dataset\n",
    "- **outcomeVar** (string) - key for the column in the dataset that contains the outcome label\n",
    "- **subPlotMatrix** (3 element int array) - dimensions of the master plot and which index to push the subplot\n",
    "- **xDim** (2 element float array) - min and max boundaries of the x-axis\n",
    "- **yDim** (2 element float array) - min and max boundaries of the y-axis\n",
    "- **colorVec** (string array) - colors to distinguish between origin datasets (e.g. train, dev,) in the subplot <br>\n",
    "- **markerVec** (string array) - markers to diistinguish between models in the subplot <br>\n",
    "\n",
    "**Outputs** <br>\n",
    "- **markerDict** (dict) - marker:model pairs.  Mostly for debug purposes\n",
    "- **colorDict** (dict) - color:dataset pairs.  Mostly for debug purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotByOutcome(performData,outcome,outcomeVar,subplotMatrix = [4,2,1],\n",
    "                  xDim = [0.4,1],yDim = [0.2,1],\n",
    "                  colorVec = ['red','blue','green','#a05195','#d45087','#f95d6a','#ff7c43','#ffa600'], \n",
    "                  markerVec = ['+','o','s','d','x','^'],debug=False):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # setup axis and graph properties\n",
    "    tempAxis = plt.subplot(subplotMatrix[0],subplotMatrix[1],subplotMatrix[2])\n",
    "    setAxisProperties(tempAxis,xDim,yDim,'Recall','Precision',outcome)\n",
    "    \n",
    "    # setup local variables \n",
    "    outcomeSubset = performData.loc[performData[outcomeVar] == outcome]  # data subset for the outcome of interest\n",
    "    dataOrigins = sorted(set(outcomeSubset['Dataset']))          # set of datasets\n",
    "    models = sorted(set(outcomeSubset['Model']))                 # set of models\n",
    "    compareLineX = [0,0]                                         # x coords for line comparing two models\n",
    "    compareLineY = [0,0]                                         # y coords for line comparing two models\n",
    "    modelsOfInterest = ['Whole','NoEmbed']                       # models to comapre\n",
    "    markerDict = {}                                              # store marker:model pairs\n",
    "    colorDict = {}                                               # store color:dataset pairs\n",
    "    \n",
    "    \n",
    "    # for each dataset and each model, plot the precision vs. recall value \n",
    "    # with the appropriate (color,symbol) type\n",
    "    for origin in dataOrigins:    \n",
    "        \n",
    "        # subset records to the current 'origin' dataset of interest\n",
    "        originSubset = outcomeSubset.loc[outcomeSubset['Dataset'] == origin]\n",
    "        datasetColor = colorVec[dataOrigins.index(origin)]\n",
    "        colorDict[origin] = datasetColor\n",
    "        \n",
    "        # for the current dataset, go thorugh each model type\n",
    "        for model in models:\n",
    "            \n",
    "            # only one datapoint for each model-dataset combination\n",
    "            dataPoint = originSubset.loc[originSubset['Model'] == model]\n",
    "            \n",
    "            # setup a line to compare difference in precision and recall \n",
    "            # between the worst- and best-performing models\n",
    "            if model in modelsOfInterest:\n",
    "                modelIndex = modelsOfInterest.index(model)\n",
    "                compareLineX[modelIndex] = float(dataPoint['Precision'])\n",
    "                compareLineY[modelIndex] = float(dataPoint['Recall'])\n",
    "            \n",
    "            \n",
    "            modelMarker = markerVec[models.index(model)]\n",
    "            markerDict[model] = modelMarker\n",
    "            plt.scatter(\n",
    "                dataPoint['Precision'], \n",
    "                dataPoint['Recall'],\n",
    "                marker = modelMarker,\n",
    "                color = datasetColor\n",
    "            )\n",
    "            \n",
    "        # add line comparing worst- and best-performing model for the current dataset\n",
    "        compare_line = mlines.Line2D(compareLineX,compareLineY,color=datasetColor)\n",
    "        tempAxis.add_line(compare_line)\n",
    "        \n",
    "        \n",
    "    # when debugging, push graph to screen rather than saving to file\n",
    "    if(debug):\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    return(colorDict,markerDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot precision vs. recall performance for all outcomes, datasets, and models ###\n",
    "Graph consists of multiple subplots, with one subplot for each outcome.  Points are colored by originating dataset (e.g. train, dev, test), while markers correspond to model (e.g. whole model, model with no emoticons, etc.)\n",
    "\n",
    "**Inputs** <br>\n",
    "- **performData** (pandas dataframe) - contains all records and labels for all outcomes.  Keys include: <br>\n",
    "    1) outcome - which outcome the record corresponds to <br>\n",
    "    2) dataset - which dataset the record corresponds to (e.g. train, dev, test, etc.) <br>\n",
    "    3) model - which model the record corresponds to <br>\n",
    "    4) precision <br>\n",
    "    5) recall <br>\n",
    "\n",
    "- **xLims** (2 element float array) - x-axis boundaries <br>\n",
    "- **yLims** (2 element float array) - y-axis boundaries <br>\n",
    "- **title** (string) - optional graph title\n",
    "- **outputFilename** (string) - absolute filepath where the plot will be saved.  If none, plot is pushed to notebook <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotPrecisionRecall(performData,xLims,yLims,title=\"Precision vs. Recall\",outputFilename=None):\n",
    "    \n",
    "    \n",
    "    # local variable setup.  \n",
    "    fig = figure(num=None, figsize=(8, 16), dpi=160, facecolor='w', edgecolor='k')\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    uniqueOutcomes = set(performData['Outcome'])\n",
    "    \n",
    "    \n",
    "    index = 1        # subplot indeces start at 1, not 0\n",
    "    colorDict, marerDict = [{} for x in range(2)]\n",
    "    \n",
    "    for outcome in uniqueOutcomes:\n",
    "        colorDict, markerDict = plotByOutcome(\n",
    "            performData,\n",
    "            outcome,\n",
    "            'Outcome',\n",
    "            [4,2,index],\n",
    "            xLims[index-1],\n",
    "            yLims[index-1]\n",
    "        )\n",
    "        index+=1\n",
    "\n",
    "    if(not outputFilename):\n",
    "        print(colorDict)\n",
    "        print(markerDict)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        plt.savefig(outputFilename,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create peformance graphs based on input CSV data.  PerformCSV corresponds to an absoluate CSV filepath\n",
    "def createPerformanceGraphs(performCSV):\n",
    "    \n",
    "    performDataset = loadPerformanceData(performCSV,True)\n",
    "    \n",
    "    # plot graphs with different x- y-axis boundaries for each outcome\n",
    "    yLims = [[0.4,1],[0.4,1],[0.5,1],[0.2,1],[0.5,1],[0.7,1],[0.0,1]]\n",
    "    xLims = [[0.4,1],[0.7,1],[0.7,1],[0.7,1],[0.7,1],[0.7,1],[0.4,1]]\n",
    "    outputFilepath = performFolder + \"precRecallVaryingBound.eps\"\n",
    "    plotPrecisionRecall(performDataset,xLims,yLims,\"Precision vs. Recall with Varying Boundaries\",outputFilepath)\n",
    "    \n",
    "    # plot graphs with the same x- and y-axisboundaries for each outcome\n",
    "    yLims = [[0.0,1],[0.0,1],[0.0,1],[0.0,1],[0.0,1],[0.0,1],[0.0,1]]\n",
    "    xLims = [[0.4,1],[0.4,1],[0.4,1],[0.4,1],[0.4,1],[0.4,1],[0.4,1]]\n",
    "    outputFilepath = performFolder + \"precRecallFixedBound.eps\"\n",
    "    plotPrecisionRecall(performDataset,xLims,yLims,\"Precision vs. Recall with Set Boundaries\",outputFilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createPerformanceGraphs(performCSV_Denorm)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
